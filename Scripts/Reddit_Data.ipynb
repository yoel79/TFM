{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "posts = pd.read_csv(\"../Data/Reddit_AAPL_Posts.csv\", index_col='index')\n",
    "comments = pd.read_csv(\"../Data/Reddit_AAPL_Comments.csv\", index_col='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesado de Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qjx7sk</td>\n",
       "      <td>2021-10-31 19:48:40</td>\n",
       "      <td>why is aapl included in dividend etfs?</td>\n",
       "      <td>I have created a package of dividend stocks by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qjx3kk</td>\n",
       "      <td>2021-10-31 19:42:55</td>\n",
       "      <td>why aapl deserves $200+ and a 35+ pe</td>\n",
       "      <td>The market has now gifted MSFT (a great but in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qjvo1l</td>\n",
       "      <td>2021-10-31 18:33:50</td>\n",
       "      <td>sell a few aapl shares and buy crm?</td>\n",
       "      <td>I've got about 5.5% AAPL and 11.1% MSFT.  I'm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qjtshr</td>\n",
       "      <td>2021-10-31 17:02:20</td>\n",
       "      <td>is $aapl able to grow much more?</td>\n",
       "      <td>I’m a broke college student but I have enough ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>qihwwl</td>\n",
       "      <td>2021-10-29 17:28:02</td>\n",
       "      <td>hot stock s-- gree, auph, x, qs, rkt, bkkt, mc...</td>\n",
       "      <td>GREE, AUPH, X, QS, RKT, BKKT, MCMJ, GEVO, SBUX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15469</th>\n",
       "      <td>5c8grj</td>\n",
       "      <td>2016-11-10 15:42:36</td>\n",
       "      <td>anyone else short aapl after their earnings lo...</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15472</th>\n",
       "      <td>5buvbp</td>\n",
       "      <td>2016-11-08 18:13:04</td>\n",
       "      <td>i have 30 shares of aapl and 20 shares of tsla...</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15474</th>\n",
       "      <td>5bat5x</td>\n",
       "      <td>2016-11-05 16:46:28</td>\n",
       "      <td>apple's eddy cue sells $37.5m in aapl stock</td>\n",
       "      <td>[deleted]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15476</th>\n",
       "      <td>5b3kvq</td>\n",
       "      <td>2016-11-04 13:36:52</td>\n",
       "      <td>i asked you fags 2 weeks ago how to short aapl...</td>\n",
       "      <td>[deleted]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15478</th>\n",
       "      <td>5ak139</td>\n",
       "      <td>2016-11-01 16:25:00</td>\n",
       "      <td>how to begin trading options (aapl)</td>\n",
       "      <td>I have both fidelity and Robin hood accounts. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7408 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id         created_utc  \\\n",
       "index                               \n",
       "0      qjx7sk 2021-10-31 19:48:40   \n",
       "1      qjx3kk 2021-10-31 19:42:55   \n",
       "3      qjvo1l 2021-10-31 18:33:50   \n",
       "4      qjtshr 2021-10-31 17:02:20   \n",
       "19     qihwwl 2021-10-29 17:28:02   \n",
       "...       ...                 ...   \n",
       "15469  5c8grj 2016-11-10 15:42:36   \n",
       "15472  5buvbp 2016-11-08 18:13:04   \n",
       "15474  5bat5x 2016-11-05 16:46:28   \n",
       "15476  5b3kvq 2016-11-04 13:36:52   \n",
       "15478  5ak139 2016-11-01 16:25:00   \n",
       "\n",
       "                                                   title  \\\n",
       "index                                                      \n",
       "0                 why is aapl included in dividend etfs?   \n",
       "1                   why aapl deserves $200+ and a 35+ pe   \n",
       "3                    sell a few aapl shares and buy crm?   \n",
       "4                       is $aapl able to grow much more?   \n",
       "19     hot stock s-- gree, auph, x, qs, rkt, bkkt, mc...   \n",
       "...                                                  ...   \n",
       "15469  anyone else short aapl after their earnings lo...   \n",
       "15472  i have 30 shares of aapl and 20 shares of tsla...   \n",
       "15474        apple's eddy cue sells $37.5m in aapl stock   \n",
       "15476  i asked you fags 2 weeks ago how to short aapl...   \n",
       "15478                how to begin trading options (aapl)   \n",
       "\n",
       "                                                selftext  \n",
       "index                                                     \n",
       "0      I have created a package of dividend stocks by...  \n",
       "1      The market has now gifted MSFT (a great but in...  \n",
       "3      I've got about 5.5% AAPL and 11.1% MSFT.  I'm ...  \n",
       "4      I’m a broke college student but I have enough ...  \n",
       "19     GREE, AUPH, X, QS, RKT, BKKT, MCMJ, GEVO, SBUX...  \n",
       "...                                                  ...  \n",
       "15469                                          [removed]  \n",
       "15472                                          [removed]  \n",
       "15474                                          [deleted]  \n",
       "15476                                          [deleted]  \n",
       "15478  I have both fidelity and Robin hood accounts. ...  \n",
       "\n",
       "[7408 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Convertir la columna 'created_utc' a tipo datetime\n",
    "posts['created_utc'] = pd.to_datetime(posts['created_utc'], unit='s')\n",
    "\n",
    "# 2. Nos quedamos solo con las columnas necesarias\n",
    "posts = posts[[ \"id\", \"created_utc\",\"title\", \"selftext\"]]\n",
    "\n",
    "# 3. Eliminar valores nulos o rellenarlos\n",
    "posts = posts.dropna(subset=['title'])\n",
    "\n",
    "# 4. Verificación final\n",
    "posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesado de Comentarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit.id</th>\n",
       "      <th>subreddit.name</th>\n",
       "      <th>subreddit.nsfw</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>permalink</th>\n",
       "      <th>body</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comment</td>\n",
       "      <td>hitr97r</td>\n",
       "      <td>2qjfk</td>\n",
       "      <td>stocks</td>\n",
       "      <td>False</td>\n",
       "      <td>1635724579</td>\n",
       "      <td>https://old.reddit.com/r/stocks/comments/qjvo1...</td>\n",
       "      <td>I own all 3.  Don't sell AAPL.</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comment</td>\n",
       "      <td>hitq83x</td>\n",
       "      <td>2qjfk</td>\n",
       "      <td>stocks</td>\n",
       "      <td>False</td>\n",
       "      <td>1635724042</td>\n",
       "      <td>https://old.reddit.com/r/stocks/comments/qj07j...</td>\n",
       "      <td>I believe TSLA want to be like AAPL: part hard...</td>\n",
       "      <td>-0.2500</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comment</td>\n",
       "      <td>hitp3wy</td>\n",
       "      <td>3qd6hq</td>\n",
       "      <td>millennialbets</td>\n",
       "      <td>False</td>\n",
       "      <td>1635723463</td>\n",
       "      <td>https://old.reddit.com/r/MillennialBets/commen...</td>\n",
       "      <td>**[Recent News for BABA-](https://www.reddit.c...</td>\n",
       "      <td>0.9422</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comment</td>\n",
       "      <td>hitnu8x</td>\n",
       "      <td>2qsbv</td>\n",
       "      <td>dividends</td>\n",
       "      <td>False</td>\n",
       "      <td>1635722829</td>\n",
       "      <td>https://old.reddit.com/r/dividends/comments/qj...</td>\n",
       "      <td>O, AAPL, NKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comment</td>\n",
       "      <td>hitmll9</td>\n",
       "      <td>2qjfk</td>\n",
       "      <td>stocks</td>\n",
       "      <td>False</td>\n",
       "      <td>1635722214</td>\n",
       "      <td>https://old.reddit.com/r/stocks/comments/qjvo1...</td>\n",
       "      <td>I have both. CRM has had better revenue and ea...</td>\n",
       "      <td>0.7896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297528</th>\n",
       "      <td>comment</td>\n",
       "      <td>d9gq1rr</td>\n",
       "      <td>2th52</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>False</td>\n",
       "      <td>1478004756</td>\n",
       "      <td>https://old.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>#Good morning traders of the r/wallstreetbets ...</td>\n",
       "      <td>0.9852</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297529</th>\n",
       "      <td>comment</td>\n",
       "      <td>d9gmf6u</td>\n",
       "      <td>2qgzg</td>\n",
       "      <td>business</td>\n",
       "      <td>False</td>\n",
       "      <td>1477995175</td>\n",
       "      <td>https://old.reddit.com/r/business/comments/5ac...</td>\n",
       "      <td>AAPL is a buy at any dips, Warren Buffet bough...</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297530</th>\n",
       "      <td>comment</td>\n",
       "      <td>d9gjouk</td>\n",
       "      <td>2qh1f</td>\n",
       "      <td>apple</td>\n",
       "      <td>False</td>\n",
       "      <td>1477984867</td>\n",
       "      <td>https://old.reddit.com/r/apple/comments/5accfe...</td>\n",
       "      <td>You must be new here.\\n\\nEveryone is disappoin...</td>\n",
       "      <td>0.5267</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297531</th>\n",
       "      <td>comment</td>\n",
       "      <td>d9gcpa1</td>\n",
       "      <td>2qjfk</td>\n",
       "      <td>stocks</td>\n",
       "      <td>False</td>\n",
       "      <td>1477969235</td>\n",
       "      <td>https://old.reddit.com/r/stocks/comments/5aepl...</td>\n",
       "      <td>Microsoft's growth opportunity has nothing to ...</td>\n",
       "      <td>0.9876</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297532</th>\n",
       "      <td>comment</td>\n",
       "      <td>d9ga8jv</td>\n",
       "      <td>2qhl1</td>\n",
       "      <td>oil</td>\n",
       "      <td>False</td>\n",
       "      <td>1477965605</td>\n",
       "      <td>https://old.reddit.com/r/oil/comments/5acul8/h...</td>\n",
       "      <td>I'm a landman. AAPL-RPL.  You can counter (and...</td>\n",
       "      <td>0.9702</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>297533 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           type       id subreddit.id  subreddit.name  subreddit.nsfw  \\\n",
       "index                                                                   \n",
       "0       comment  hitr97r        2qjfk          stocks           False   \n",
       "1       comment  hitq83x        2qjfk          stocks           False   \n",
       "2       comment  hitp3wy       3qd6hq  millennialbets           False   \n",
       "3       comment  hitnu8x        2qsbv       dividends           False   \n",
       "4       comment  hitmll9        2qjfk          stocks           False   \n",
       "...         ...      ...          ...             ...             ...   \n",
       "297528  comment  d9gq1rr        2th52  wallstreetbets           False   \n",
       "297529  comment  d9gmf6u        2qgzg        business           False   \n",
       "297530  comment  d9gjouk        2qh1f           apple           False   \n",
       "297531  comment  d9gcpa1        2qjfk          stocks           False   \n",
       "297532  comment  d9ga8jv        2qhl1             oil           False   \n",
       "\n",
       "        created_utc                                          permalink  \\\n",
       "index                                                                    \n",
       "0        1635724579  https://old.reddit.com/r/stocks/comments/qjvo1...   \n",
       "1        1635724042  https://old.reddit.com/r/stocks/comments/qj07j...   \n",
       "2        1635723463  https://old.reddit.com/r/MillennialBets/commen...   \n",
       "3        1635722829  https://old.reddit.com/r/dividends/comments/qj...   \n",
       "4        1635722214  https://old.reddit.com/r/stocks/comments/qjvo1...   \n",
       "...             ...                                                ...   \n",
       "297528   1478004756  https://old.reddit.com/r/wallstreetbets/commen...   \n",
       "297529   1477995175  https://old.reddit.com/r/business/comments/5ac...   \n",
       "297530   1477984867  https://old.reddit.com/r/apple/comments/5accfe...   \n",
       "297531   1477969235  https://old.reddit.com/r/stocks/comments/5aepl...   \n",
       "297532   1477965605  https://old.reddit.com/r/oil/comments/5acul8/h...   \n",
       "\n",
       "                                                     body  sentiment  score  \n",
       "index                                                                        \n",
       "0                          I own all 3.  Don't sell AAPL.     0.0000      1  \n",
       "1       I believe TSLA want to be like AAPL: part hard...    -0.2500      2  \n",
       "2       **[Recent News for BABA-](https://www.reddit.c...     0.9422      1  \n",
       "3                                            O, AAPL, NKE        NaN      1  \n",
       "4       I have both. CRM has had better revenue and ea...     0.7896      1  \n",
       "...                                                   ...        ...    ...  \n",
       "297528  #Good morning traders of the r/wallstreetbets ...     0.9852      1  \n",
       "297529  AAPL is a buy at any dips, Warren Buffet bough...     0.4215      3  \n",
       "297530  You must be new here.\\n\\nEveryone is disappoin...     0.5267     -1  \n",
       "297531  Microsoft's growth opportunity has nothing to ...     0.9876      3  \n",
       "297532  I'm a landman. AAPL-RPL.  You can counter (and...     0.9702      2  \n",
       "\n",
       "[297533 rows x 10 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hitr97r</td>\n",
       "      <td>2021-10-31</td>\n",
       "      <td>I own all 3.  Don't sell AAPL.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hitq83x</td>\n",
       "      <td>2021-10-31</td>\n",
       "      <td>I believe TSLA want to be like AAPL: part hard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hitp3wy</td>\n",
       "      <td>2021-10-31</td>\n",
       "      <td>**[Recent News for BABA-](https://www.reddit.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hitnu8x</td>\n",
       "      <td>2021-10-31</td>\n",
       "      <td>O, AAPL, NKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hitmll9</td>\n",
       "      <td>2021-10-31</td>\n",
       "      <td>I have both. CRM has had better revenue and ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297528</th>\n",
       "      <td>d9gq1rr</td>\n",
       "      <td>2016-11-01</td>\n",
       "      <td>#Good morning traders of the r/wallstreetbets ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297529</th>\n",
       "      <td>d9gmf6u</td>\n",
       "      <td>2016-11-01</td>\n",
       "      <td>AAPL is a buy at any dips, Warren Buffet bough...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297530</th>\n",
       "      <td>d9gjouk</td>\n",
       "      <td>2016-11-01</td>\n",
       "      <td>You must be new here.\\n\\nEveryone is disappoin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297531</th>\n",
       "      <td>d9gcpa1</td>\n",
       "      <td>2016-11-01</td>\n",
       "      <td>Microsoft's growth opportunity has nothing to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297532</th>\n",
       "      <td>d9ga8jv</td>\n",
       "      <td>2016-11-01</td>\n",
       "      <td>I'm a landman. AAPL-RPL.  You can counter (and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>297533 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id created_utc                                               body\n",
       "index                                                                         \n",
       "0       hitr97r  2021-10-31                     I own all 3.  Don't sell AAPL.\n",
       "1       hitq83x  2021-10-31  I believe TSLA want to be like AAPL: part hard...\n",
       "2       hitp3wy  2021-10-31  **[Recent News for BABA-](https://www.reddit.c...\n",
       "3       hitnu8x  2021-10-31                                       O, AAPL, NKE\n",
       "4       hitmll9  2021-10-31  I have both. CRM has had better revenue and ea...\n",
       "...         ...         ...                                                ...\n",
       "297528  d9gq1rr  2016-11-01  #Good morning traders of the r/wallstreetbets ...\n",
       "297529  d9gmf6u  2016-11-01  AAPL is a buy at any dips, Warren Buffet bough...\n",
       "297530  d9gjouk  2016-11-01  You must be new here.\\n\\nEveryone is disappoin...\n",
       "297531  d9gcpa1  2016-11-01  Microsoft's growth opportunity has nothing to ...\n",
       "297532  d9ga8jv  2016-11-01  I'm a landman. AAPL-RPL.  You can counter (and...\n",
       "\n",
       "[297533 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Convertir la columna 'created_utc' a tipo datetime\n",
    "comments['created_utc'] = pd.to_datetime(comments['created_utc'])\n",
    "\n",
    "# 2. Nos quedamos solo con las columnas necesarias\n",
    "comments = comments[['id', 'created_utc', 'body']]\n",
    "\n",
    "# 3. Eliminar valores nulos o rellenarlos\n",
    "comments = comments.dropna(subset=[\"body\"])\n",
    "\n",
    "# 4. Verificación final\n",
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKMAAAJOCAYAAABr8MR3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABpmklEQVR4nO3deVwVZf//8fcRFBUBtwBRVHLBBdNcUtyt3ErNpaw00zIrNc3M2zQrsUy7/XZbpq22WGnZalp2a1rua5qomFu5L7gkgguiwPz+8MfcHuEIBw5zDofX8/HgkcxcM/OZOXMdzry7Zo7NMAxDAAAAAAAAgAWKuLsAAAAAAAAAFB6EUQAAAAAAALAMYRQAAAAAAAAsQxgFAAAAAAAAyxBGAQAAAAAAwDKEUQAAAAAAALAMYRQAAAAAAAAsQxgFAAAAAAAAy/i6uwAAAAAA3uvrr7/Wn3/+qY4dOyo6Otrd5QAAPAAjowAAKEBsNptsNpvTy7Vt21Y2m03Lly93fVFZyG2dN2L1PqDgqVq1qmw2mw4cOODuUvD//fbbb+rTp4+WLVumhg0burscAICHIIwCABQaa9as0eOPP65atWopKChIfn5+qlixorp06aIPP/xQFy5ccGt9s2bNUkxMDBfSHsbTzxtPsnz5csXExBAYFnAHDhwwA+WMH19fX5UtW1bVq1dXz549NXXqVJ06deqG6zl+/Lj69OmjyMhIzZ8/X35+fhbtAQDA0xFGAQC83sWLF3X//ferZcuWmjlzpg4ePKjw8HDdcsstMgxDCxcu1KBBg1SjRg1t377dbXXOmjVLEyZMuGEYFRkZqcjISOuKKsQKynnjSZYvX64JEya4LYyqVq2aIiMjVbRoUbds3xs1btxYLVq0ULNmzVS5cmWdO3dO8+bN07PPPqtKlSpp/PjxSktLy7RcWlqaHnjgAfn4+GjRokUqXbq09cUDADwWYRQAwKtduXJFHTp00Ndff63Q0FB9+umnOnPmjOLi4vT777/r2LFj2rFjh5544gmdOnVKf//9t7tLvqFdu3Zp165d7i7D63nbeVNY/Prrr9q1a5cqVqzo7lK8xjfffKPVq1dr9erVio2N1YkTJ7R3716NHDlSaWlpevnll9WvX79My+3atUvt2rXT4sWLFR4e7obKAQCejAeYAwC82oQJE7RmzRqFhIRo3bp1qlq1aqY2derU0XvvvaeHHnpIRYrw/2nAeQPcSPXq1fWf//xH7du3V5cuXfTll1+qY8eO6t+/v9mmbt26qlu3rhurBAB4Mj45AQC8VmJiot566y1J0ptvvplloHCtli1bqnnz5ubvycnJ+vLLL/XAAw8oMjJSpUqVUqlSpdSgQQNNnDjR4bOCrn2I8vr169W5c2eVKVNG/v7+atWqlX777Te79suXL5fNZtOKFSskSe3atbN7VsusWbPMtjd6MPjp06c1ZMgQVaxYUcWLF1dkZKReeeUVXblyxeE+x8fHa/r06erYsaOqVq2q4sWLq0yZMmrTpo0+//zzGx6vbdu26Z577lGZMmVUqlQpNW3aVHPnzr3hMtnJzT5kWLx4sbp166aQkBD5+fmpUqVKeuSRR5wetZTX8ybDwoUL1alTJ5UvX15+fn6KiIjQkCFDdPjw4SzXc+15s2LFCt15550qXbq0ypYtqx49emjv3r1m2wULFqhVq1YKDAxUmTJl9OCDD+rYsWMOazxz5ozGjRunqKgo+fv7KyAgQM2aNdPMmTOVnp6eqf2AAQPMc+/YsWN69NFHVaFCBRUvXlx169bV22+/nWkZm82mCRMmSLoa5l17Dg8YMMBst2/fPv373/9W27ZtFR4eLj8/P910003q1KmTFi5cmGX9GX2kbdu2Sk1N1ZQpU1SvXj2VLFnS7vW50QPMr1y5ounTp+u2225TYGCg/P39Vb9+fb366qu6ePFiltuNi4tT3759FR4ermLFiql06dKqUaOG+vTpo0WLFjk83tebNWuWeRzOnTunkSNHmv3t5ptv1rhx4xzWIEk7duxQv379VKlSJRUrVkwhISHq1auX1q9fn2X7a1+//fv3a8CAAapYsaJ8fX0VExOT47qz06lTJz311FOSpMmTJ2ea7+j9KjfnAADAyxgAAHipOXPmGJKMm266ybhy5YrTy69atcqQZPj6+hqVKlUyGjdubNSoUcPw9fU1JBkNGzY0Ll68mGm5KlWqGJKM6dOnG0WLFjXKlStnNGrUyAgKCjLXt2zZMrP9H3/8YbRo0cIIDAw0JBlRUVFGixYtzJ+ff/7ZbCvJyOrP9/Hjx42bb77ZXH+DBg2MGjVqGJKMLl26GK1btzYk2W3XMAzjlVdeMSQZJUqUMKpVq2Y0btzYqFy5srmdJ598Mstjs2LFCqNEiRKGJCMwMNBo3LixERoaakgypkyZ4rDOG8ntPhiGYTz99NPmNoODg41bb73VPJ6BgYHGmjVrclxHXs8bwzCMMWPGmPVUqlTJaNSokVGyZElDklGmTBnj999/z7RMxnkzdepUw8fHxwgODjYaNmxo+Pv7G5KMChUqGMePHzemTp1qrrd+/fqGn5+fIcmIjIw0kpOTM603Li7OqFixoiHJKFasmFGnTh2jWrVqhs1mMyQZ9957r5Genm63TP/+/Q1JRkxMjBEaGmoUL17caNiwoREWFmbu18SJE+2WadGihREeHm5IMsLDw+3O4VdffdVsN3DgQEOSUapUKaNmzZpG48aNjQoVKpjrfe211zLtw7JlywxJRuvWrY27777bkGRUq1bNaNSokVG3bt1Mx3D//v12y1+8eNG4/fbbzW3Url3buOWWW4wiRYoYkowGDRoYp0+ftltmw4YN5jkeFBRk1K9f34iKijL78T333OPw9b/eJ598YkgyHnjgAePWW281bDabUbduXSMqKsp8HZo1a2ZcuHAh07Lz5883X+PSpUsbjRs3Nm666SZDklGkSBHjgw8+yLRMxus3ZswYo3Tp0oafn5/RsGFDo1atWkZMTEy29e7fv988Vtcfy+vt3LnTbPvXX3/ZzXP0PpCbcwAA4F0IowAAXmvo0KGGJKN79+65Wv7AgQPG119/bZw7d85u+vHjx417773XvFi/XsYFcdGiRY3JkycbqamphmEYxuXLl42+ffsakoymTZtmWq5NmzYOw5YMji7uevToYQZkhw4dMqf/+uuvRkBAgFG0aNEs171q1Srjt99+M2vMsHXrVqN27dqGJGP58uV2886fP29UqlTJkGQ8/PDD5gV0Wlqa8Z///MfclrNhVG734b333jMkGREREXbzUlNTjYkTJ5rBTVZBTVbyet78+OOPZqA2e/Zsc3piYqK5j1WrVs0UZF573vznP/8x0tLSDMMwjISEBKNZs2aGJOPuu+82SpYsacyZM8dc7tChQ2aI984779it8/z580a1atUMScbw4cONxMREc96OHTuMunXrGpKMGTNm2C2XEWYULVrUuPfee42EhARz3jvvvGNIMooXL2433TAMY/z48YYkY/z48Q6Pz88//2ysX78+UwC2cuVKo0KFCoaPj0+mUCMjjMoI6dauXWvOu/Z1dRRGPfvss4YkIywszNi8ebM5fe/evUatWrUMSUbv3r3tlunSpYshyXj++eeNlJQUu3m///673WuQnYwwytfX16hYsaIRGxtrztu+fbsZ4o0aNcpuuaNHj5qh6tNPP23WkZaWZrz66qvma7R161a75TJePx8fH6Nbt27GP//8k+XxcsSZMMowDKNcuXKGJOPLL7+0m+7ofSA35wAAwLsQRgEAvFb37t0NScYzzzzj8nVfvHjRKFasmFGjRo1M8zIuiLt27Zpp3qlTp8xRDmfOnLGbl9swau/eveboiri4uEzLZIykyW7d11u6dKkhyRg0aJDd9A8//NCQZFSsWNG4fPlypuW6devmdBiV231ISUkxQkNDDR8fH+OPP/7Ict29evUyJBmfffZZjmrJ63nTokULMzy43oULF4zy5csbkoyPPvrIbl7GeZPViJvFixeb+5/VejMCuW7dutlNf+uttwxJRo8ePbKsdevWrYbNZjNuvvlmu+kZYUZoaKhx/vz5TMs1bNjQkGR8//33dtNzEkbdSMa5de1IKsP4Xxglyfjuu+8cLp9VGJWYmGiOSps3b16mZTZu3GhIMmw2m10AEhkZaUiyC/ByKyOMyuqYGYZhLFiwwJBk+Pv7G0lJSeb0cePGmSO3snLXXXcZkox+/frZTc/u9cuOs2FUgwYNDEnGtGnT7KbnJpR2dA4AALwLz4wCAHitc+fOSZL8/f1zvY709HTNnz9fQ4cOVefOndWqVSu1bNlS7du3l81m0969ex0+6+Wxxx7LNK18+fLmM2727duX67qu9csvv8gwDLVu3TrLBwY/9thjKlasmMPlz507p5kzZ6p///7q0KGDuY9jxoyRJG3dutWu/eLFiyVJAwcOVNGiRTOtb8iQIZbtw7p16xQfH6+GDRvq1ltvzXLd3bp1kyTzmVzZyct5c/78ea1bt06SNGzYsEzzS5YsqUGDBkm6us9ZGThwYKZpDRo0uOH8jH2//pz6/vvvJWV9LkrSLbfcoqpVq2rfvn06cuRIpvkPPvhglsehSZMmWW4vp06dOqVp06apT58+uvPOO9WyZUu1bNlSb775pqTM51yGoKAg3XPPPU5ta/Xq1bp48aIqV66c5bJNmjRRdHS0DMPQkiVLzOkZ3wD39ddfO7W9G6lYsWKWNXTp0kWVK1fWhQsXtGbNGnN6xjmS8Vym6z399NN27a7Xq1evPL3/5VTGNjL6Tk7k9hwAAHgHvk0PAOC1AgICJMnhg8azc/bsWd11111muOBIQkKCSpYsmWl6tWrVsmwfHBys3bt36/z587mq63p79uyRJNWuXTvL+QEBAapYsaL279+fad6WLVvUpUuXbB9+7cz2HE2/kdzuw/bt2yVJBw4cUMuWLbNc9uzZs5Kko0eP5qiWvJw3f/31l9LT0+Xn56ebb745yzYZYVvGPl8vq/PmpptuytH868+pjOPz0ksvadKkSVlu7/Tp05KuHp9KlSplW4t09RzOans58csvv6h3795KTEx02Ob6cy5DjRo15OPj49T2Mo5zrVq1HD78v27dulq3bp3dazJixAgtXbpUgwYN0n/+8x917NhRLVu2VLt27VSuXDmnasgQGRmZ5Tcv2mw2RUZG6tChQ9qzZ486depkV3udOnUc1i1JJ06cUFJSkgIDA+3m56Yv5kbGeXD99h3JyzkAAPAOjIwCAHitihUrSlKWIUxOjBw5UuvWrVNkZKS+++47HT16VCkpKTKu3uZurt/RN705GpGQcTFqGEau6rpexoXgtYHF9UJCQjJNS0tLU+/evXXs2DHdddddWrFihU6fPq3U1FQZhmF+e9v1+5fd9rLaVn7tQ8bF7KlTp7RmzZosf3bs2CHp6rcj5kRezptr98NR8JGxH45GkWQVbF67rhvNv/6cyjg+mzdvdnh8MurI6vi4+hw+e/asHnjgASUmJurhhx/W+vXrlZCQoLS0NLuRSc72qRvJeE0yArSsZPWa3H333Vq4cKGaN2+uPXv2aNq0abrvvvsUGhqq3r175zjcvJazNWRX+7V9IqvzyYpRUZLMb4i80f5lyOs5AADwDoRRAACv1bx5c0nS2rVrlZqa6tSyqamp5u058+fPV8+ePRUWFmbeKpaamqr4+HjXFpxLpUqVknQ1kHHk5MmTmaZt3LhRf/31l6pUqaLvv/9erVu3Vrly5cyRJxkXmM5uL6ttZSe3+5CxXN++fc2Q0NHP8uXLc1RLXs6ba/fDUVBz4sQJSf8bgZWfMurZu3dvtsenbdu2+V7Pf//7XyUkJCg6OlqzZs1S06ZNVbp0aTPccnTO5UXGMbjReenoNbnrrru0Zs0anTp1Sj/88IOGDRum0qVL65tvvlHXrl2dDkxycn5fW0N2tWfUnVXtVvnzzz/NUUy33XZbtu3dcQ4AADwPYRQAwGvdddddKlWqlE6ePKlvv/3WqWVPnTqlCxcuqGzZsoqMjMw0Py4uTmlpaa4qVZIcjqTJTs2aNSVJu3btynL++fPns3we0IEDByRJjRo1kp+fX6b5jp7Zkt32du7cmW3Nzq7T0T5k3L4UFxfn9DYdyct5U716dRUpUkQpKSkOn6eUMVIrY5/zU34cnxvJ7hzOOOeio6OzbJsfzwnKOM47d+50GBBm95qULVtW99xzj9566y3FxcUpKChIW7Zs0aZNm5yqZffu3UpPT8803TAM7d69O1MNGf/+888/b1h3SEhIjm+Rc7X33ntP0tVbAiMiIrJt745zAADgeQijAABeq3Tp0uZDpEeMGGFeBDmyZs0arV27VpJUokQJSVJSUlKWty9NmTLFtcVes82c3k6WoUOHDpKklStXZnnR+uGHH+ry5csOt3ft6IoMV65cMR8k7Gh7H330UZYjQ955550c1379Op3dh1atWql8+fLaunVrjkc+ZScv502pUqXMkVXTp0/P1DY5OVkffvihJKljx44uqfdGevbsKUl66623XHZb6I1kdw7f6Jz7559/9NFHH7m8ppYtW6pkyZI6fPiw5s+fn2n+pk2btG7dOtlsNrVv3z7b9YWEhJihy42etZaVI0eO6Mcff8w0feHChTp48KD8/f3VokULc3rGOTJjxows1/fWW2/ZtbPaokWLzP7+/PPP52gZd5wDAADPQxgFAPBqMTExio6O1okTJxQdHa3PP/9cly5dsmuzZ88eDR06VG3btjVvhyldurTq1q2r1NRUPfPMM2YQkpaWpn//+9/66quvbvgNdbmR8cDrnH7rW4bq1avrnnvukWEY6t+/v90IouXLlysmJibLb71r1qyZfH19tWbNGn322Wfm9MTERPXt2zfLi0Xp6jesVaxYUUeOHNETTzxhBg+GYWjatGn6+eefnao/L/tQvHhxvfzyy5Kk++67T/PmzcsUusTFxem5556z+5ay7OT2vJGk5557TtLVUO6LL74wp587d04PP/ywTp06papVq+qBBx7IcT259cQTT+jmm2/WsmXL1LdvXx0/ftxu/vnz5/X1119r5MiRLtlexjns6BbHVq1aSbr6DXVLly41px8/fly9evVy+rbInAgMDNTgwYMlXf1Wui1btpjz/v77b/Xv31+S1Lt3b7sHtj/wwANauHBhphD022+/1fbt22Wz2Rx+g6Mjvr6+GjZsmPlgeenqqKeMb8t78skn7W63Gzx4sAIDAxUbG2v3PpSenq4pU6Zo4cKFKlq0qJ599lmn6sirv/76S88++6y6dOmitLQ0PfTQQ3rooYdytKw7zgEAgAcyAADwcufOnTN69eplSDIkGSVKlDCioqKMJk2aGBUrVjSnV6pUydi+fbu53IIFCwybzWZIMsqWLWs0btzYKF++vCHJePHFF40qVaoYkoz9+/fbbc/R9Axt2rQxJBnLli2zm75y5Uqzlpo1axqtW7c22rRpY/z3v/8122TMv97Ro0eNqlWrGpKMokWLGrfeeqtRs2ZNQ5Jx9913G61bt85ym6NGjTLXWblyZaNRo0ZGiRIljKJFixrvvvuuIcmoUqVKpu399ttvhp+fnyHJCAwMNJo0aWKEhoYakowpU6Y4rPNGcrsPhmEYY8aMMbdZtmxZo0mTJkbDhg2NsmXLmtOvPY45kdvz5vp6wsPDjcaNGxv+/v6GJKNMmTLGxo0bM20vu/PmRsd0//79Dl+rnTt3GhEREYYko0iRIkbt2rWNpk2bGjVr1jR8fHwMSUbTpk3tlunfv78hyfjkk0+y3N748eMNScb48ePtpicmJhplypQxJBkVKlQwWrRoYbRp08aYPHmy2ebee+8196V69epGgwYNDF9fXyMgIMB48803DUlGmzZt7Na7bNmyLKdfz9ExvHjxotGuXTtzu3Xq1DHq169v7n/9+vWN06dP2y0TFBRkSDL8/PzM171ChQrmOl588cUb1nKtTz75xJBkPPDAA8att95q2Gw2IyoqyqhXr575HtOkSRPj/PnzmZadP3++UaxYMfPcadKkiREcHGy+nu+//36mZbJ7/bKTcT5JMho3bmy0aNHCaNGihdGgQQNz25KMYsWKGTExMUZqamqW63F0zubmHAAAeBdGRgEAvF6pUqX07bffauXKlRo4cKDCw8N14MABbd26VYZh6O6779ZHH32kPXv2KCoqylyua9eu+u9//6vmzZsrOTlZu3fvVvXq1TV79mxzNI4rtWrVSl988YVuu+02HT16VCtXrtSKFSty9KD0sLAwbdy4UU8++aTKly+vP//8U4Zh6OWXX9a8efMcPstnypQpevPNN1WrVi3Fx8fr4MGDuvPOO7Vq1Srz6+Wz0q5dO61fv15du3aVzWbTn3/+qfDwcH355Zf617/+lav9z+0+SNLkyZO1Zs0a9enTR/7+/tq6dasOHDigSpUq6dFHH9XChQt1xx13OFVPbs+bjHp+/PFHtW/fXufPn9e2bdtUvnx5Pfnkk9q6dauaNGmSq2OUG7Vq1dLWrVv12muvqUmTJjp69KhiY2N1+fJltWnTRq+//rrmzp3rkm0FBgbql19+UefOnZWSkqJ169ZpxYoVds8CmzNnjl588UVVrVpVBw8eVHx8vO699179/vvvql+/vkvquF6JEiW0ePFiTZs2TY0bN9bBgwe1Z88e1alTRxMnTtTatWtVrlw5u2U+/fRTPf7446pRo4aOHTumbdu2qWTJkurRo4dWrFiRq/cAPz8/rVixQk8//bSSkpK0e/duVa5cWWPGjNGyZcuy/Pa7bt26afPmzerbt6+KFy+u2NhYGYahHj16aPXq1Xr88cdzfVxyYtOmTVqzZo3WrVunAwcOKCAgQD169NDUqVN15MgRjR8/3vzSg5xyxzkAAPAsNsOw4AECAAAAQCE1a9YsPfLII+rfv79mzZrl7nIAAHA7RkYBAAAAAADAMoRRAAAAAAAAsAxhFAAAAAAAACxDGAUAAAAAAADL8ABzAAAAAAAAWIaRUQAAAAAAALAMYRQAAAAAAAAs4+vuAgqS9PR0HTt2TAEBAbLZbO4uBwAAAAAAwO0Mw9C5c+cUFhamIkWyH/dEGOWEY8eOKTw83N1lAAAAAAAAeJzDhw+rUqVK2bYjjHJCQECApKsHNzAw0M3VAAAAAAAAuF9SUpLCw8PN3CQ7hFFOyLg1LzAwkDAKAAAAAADgGjl9pBEPMAcAAAAAAIBlCKMAAAAAAABgGcIoAAAAAAAAWIYwCgAAAAAAAJYhjAIAAAAAAIBlCKMAAAAAAABgGcIoAAAAAAAAWIYwCgAAAAAAAJYhjAIAAAAAAIBlCKMAAAAAAABgGcIoAAAAAAAAWIYwCgAAAAAAAJYhjAIAAAAAAIBlCKMAAAAAAABgGcIoAAAAAAAAWIYwCgAAAAAAAJYhjAIAAAAAAIBlCKMAAAAAAABgGcIoAAAAAAAAWIYwCgAAAAAAAJYhjAIAAAAAAIBlCKMAAAAAAABgGcIoAAAAAAAAWIYwCgAAAAAAAJYhjAIAAAAAAIBlCKMAAAAAAABgGcIoAAAAAAAAWIYwCgAAAAAAAJYhjAIAAAAAAIBlCKMAAAAAAABgGcIoAAAAAAAAWIYwCgAAAAAAAJYhjAIAAAAAAIBlCKMAAAAAAABgGcIoAAAAAAAAWIYwCgAAAAAAAJYhjAIAAAAAAIBlCKMAAAAAAABgGcIoAAAAAAAAWIYwCgAAAAAAAJYhjAIAAAAAAIBlCKMAAAAAAABgGcIoAAAAAAAAWIYwCgAAAAAAAJYhjAIAAAAAAIBlCKMAAAAAAABgGcIoAAAAAAAAWIYwCgAAAAAAAJYhjAIAAAAAAIBlCKMAAAAAAABgGbeHUZMnT1aTJk0UEBCg4OBgde/eXbt377ZrM2DAANlsNrufZs2a2bVJSUnRsGHDVL58efn7+6tbt246cuSIXZuEhAT169dPQUFBCgoKUr9+/XT27Nn83kUAAAAAAAD8f24Po1asWKGhQ4dq/fr1WrJkiVJTU9WhQwdduHDBrl2nTp10/Phx8+fnn3+2mz9ixAjNmzdPc+fO1erVq3X+/Hl16dJFaWlpZps+ffooNjZWixYt0qJFixQbG6t+/fpZsp8AAAAAAACQbIZhGO4u4lqnTp1ScHCwVqxYodatW0u6OjLq7Nmz+uGHH7JcJjExUTfddJM+//xz3X///ZKkY8eOKTw8XD///LM6duyonTt3qk6dOlq/fr2aNm0qSVq/fr2io6O1a9cuRUZGZltbUlKSgoKClJiYqMDAQNfsMAAAAAAAQAHmbF7i9pFR10tMTJQklS1b1m768uXLFRwcrJo1a2rQoEE6efKkOW/z5s26cuWKOnToYE4LCwtTVFSU1q5dK0lat26dgoKCzCBKkpo1a6agoCCzzfVSUlKUlJRk9wMAAAAAAIDc86gwyjAMjRw5Ui1btlRUVJQ5vXPnzpozZ45+++03/ec//9Hvv/+u22+/XSkpKZKk+Ph4FStWTGXKlLFbX0hIiOLj4802wcHBmbYZHBxstrne5MmTzedLBQUFKTw83FW7CgAAAAAAUCj5uruAaz311FPatm2bVq9ebTc949Y7SYqKilLjxo1VpUoVLVy4UD179nS4PsMwZLPZzN+v/bejNtcaO3asRo4caf6elJREIAUAAAAAAJAHHjMyatiwYVqwYIGWLVumSpUq3bBthQoVVKVKFe3du1eSFBoaqsuXLyshIcGu3cmTJxUSEmK2OXHiRKZ1nTp1ymxzPT8/PwUGBtr9AAAAAAAAIPfcHkYZhqGnnnpK33//vX777TdFRERku8w///yjw4cPq0KFCpKkRo0aqWjRolqyZInZ5vjx44qLi1Pz5s0lSdHR0UpMTNTGjRvNNhs2bFBiYqLZBgAAAAAAAPnL7d+mN2TIEH3xxReaP3++3TfaBQUFqUSJEjp//rxiYmLUq1cvVahQQQcOHNDzzz+vQ4cOaefOnQoICJAkDR48WD/99JNmzZqlsmXLatSoUfrnn3+0efNm+fj4SLr67Kljx47p/ffflyQ9/vjjqlKlin788ccc1cq36QEAAAAAANhzNi9xexjl6HlNn3zyiQYMGKDk5GR1795dW7Zs0dmzZ1WhQgW1a9dOr7zyit3zmy5duqR//etf+uKLL5ScnKw77rhD77zzjl2bM2fOaPjw4VqwYIEkqVu3bpoxY4ZKly6do1oJowAAAAAAAOwVuDCqICGMAgAAAAAAsOdsXuL2Z0YBAAAAAACg8CCMAgAAAAAAgGUIowAAAAAAAGAZwigAAAAAAABYhjAKAAAAAAAAliGMAgAAAAAAgGUIowAAAAAAAGAZwigAAAAAAABYhjAKAAAAAAAAliGMAgAAAAAAgGUIowAAAAAAAGAZwigAAAAAAABYhjAKAAAAAAAAliGMAgAAAAAAgGUIowAAAAAAAGAZwigAAAAAAABYhjAKAAAAAAAAliGMAgAAAAAAgGUIowAAAAAAAGAZwigAAAAAAABYhjAKAAAAAAAAliGMAgAAAAAAgGUIowAAAAAAAGAZwigAAAAAAABYhjAKAAAAAAAAliGMAgAAAAAAgGUIowAAAAAAAGAZwigAAAAAAABYhjAKAAAAAAAAliGMAgAAAAAAgGUIowAAAAAAAGAZwigAAAAAAABYhjAKAAAAAAAAliGMAgAAAAAAgGUIowAAAAAAAGAZwigAAAAAAABYhjAKAAAAAAAAliGMAgAAAAAAgGUIowAAAAAAAGAZwigAAAAAAABYhjAKAAAAAAAAliGMAgAAAAAAgGUIowAAAAAAAGAZwigAAAAAAABYhjAKAAAAAAAAliGMAgAAAAAAgGUIowAAAAAAAGAZwigAAAAAAABYhjAKAAAAAAAAliGMAgAAAAAAgGUIowAAAAAAAGAZwigAAAAAAABYhjAKAAAAAAAAliGMAgAAAAAAgGUIowAAAAAAAGAZwigAAAAAAABYhjAKAAAAALzYa1tOu7sEALBDGAUAAAAAAADLEEYBAAAAAADAMoRRAAAAAAAAsAxhFAAAAAB4KZ4XBcATEUYBAAAAAADAMoRRAAAAAAAAsAxhFAAAAAAAACxDGAUAAAAAAADLEEYBAAAAAADAMoRRAAAAAAAAsAxhFAAAAAAAACxDGAUAAAAAAADLEEYBAAAAAADAMoRRAAAAAAAAsAxhFAAAAAAAACxDGAUAAAAAAADLuD2Mmjx5spo0aaKAgAAFBwere/fu2r17t10bwzAUExOjsLAwlShRQm3bttWOHTvs2qSkpGjYsGEqX768/P391a1bNx05csSuTUJCgvr166egoCAFBQWpX79+Onv2bH7vIgAAAAAAAP4/t4dRK1as0NChQ7V+/XotWbJEqamp6tChgy5cuGC2mTJliqZOnaoZM2bo999/V2hoqNq3b69z586ZbUaMGKF58+Zp7ty5Wr16tc6fP68uXbooLS3NbNOnTx/FxsZq0aJFWrRokWJjY9WvXz9L9xcAAAAA8tNrW067uwQAuCGbYRiGu4u41qlTpxQcHKwVK1aodevWMgxDYWFhGjFihJ577jlJV0dBhYSE6N///reeeOIJJSYm6qabbtLnn3+u+++/X5J07NgxhYeH6+eff1bHjh21c+dO1alTR+vXr1fTpk0lSevXr1d0dLR27dqlyMjIbGtLSkpSUFCQEhMTFRgYmH8HAQAAAABy6bUtpzXm1vLmvyWZvwNAfnA2L3H7yKjrJSYmSpLKli0rSdq/f7/i4+PVoUMHs42fn5/atGmjtWvXSpI2b96sK1eu2LUJCwtTVFSU2WbdunUKCgoygyhJatasmYKCgsw210tJSVFSUpLdDwAAAAAAAHLPo8IowzA0cuRItWzZUlFRUZKk+Ph4SVJISIhd25CQEHNefHy8ihUrpjJlytywTXBwcKZtBgcHm22uN3nyZPP5UkFBQQoPD8/bDgIAAAAAABRyHhVGPfXUU9q2bZu+/PLLTPNsNpvd74ZhZJp2vevbZNX+RusZO3asEhMTzZ/Dhw/nZDcAAAAAAADggMeEUcOGDdOCBQu0bNkyVapUyZweGhoqSZlGL508edIcLRUaGqrLly8rISHhhm1OnDiRabunTp3KNOoqg5+fnwIDA+1+AAAAAAAAkHtuD6MMw9BTTz2l77//Xr/99psiIiLs5kdERCg0NFRLliwxp12+fFkrVqxQ8+bNJUmNGjVS0aJF7docP35ccXFxZpvo6GglJiZq48aNZpsNGzYoMTHRbAMAAAAAAID85evuAoYOHaovvvhC8+fPV0BAgDkCKigoSCVKlJDNZtOIESM0adIk1ahRQzVq1NCkSZNUsmRJ9enTx2w7cOBAPfvssypXrpzKli2rUaNGqV69errzzjslSbVr11anTp00aNAgvf/++5Kkxx9/XF26dMnRN+kBAAAAAAAg79weRr377ruSpLZt29pN/+STTzRgwABJ0ujRo5WcnKwhQ4YoISFBTZs21S+//KKAgACz/RtvvCFfX1/17t1bycnJuuOOOzRr1iz5+PiYbebMmaPhw4eb37rXrVs3zZgxI393EAAAAAAAACabYRiGu4soKJKSkhQUFKTExESeHwUAAADAI7225bTG3Fre/Lck83cAyA/O5iVuf2YUAAAAAAAACg/CKAAAAAAAAFiGMAoAAAAAAACWIYwCAAAAAACAZQijAAAAAAAAYBnCKAAAAAAAAFiGMAoAAAAAAACWIYwCAAAAAACAZQijAAAAAAAAYBnCKAAAAAAAAFiGMAoAAAAAAACWIYwCAAAAAACAZQijAAAAAAAAYBnCKAAAAAAAAFiGMAoAAAAAAACWIYwCAAAAAACAZQijAAAAAAAAYBmXhFGXLl3Srl27lJaW5orVAQAAAAAAwEs5HUZNnz5dr7zyivn75s2bFR4errp166pmzZo6fPiwSwsEAAAAAACA93A6jPrwww9VunRp8/fnnntOZcuW1RtvvCHDMDRx4kRX1gcAAAAAAAAv4uvsAocOHVKtWrUkSefOndPKlSs1d+5c9ezZU2XKlNFLL73k8iIBAAAAAADgHZweGZWSkqKiRYtKktatW6f09HTdeeedkqSqVasqPj7etRUCAAAAAADAazgdRlWuXFmrVq2SJM2fP18NGjRQYGCgJOnUqVPmvwEAAAAAAIDrOX2b3kMPPaQJEybohx9+0NatW/X666+b8zZt2qSaNWu6tEAAAAAAAAB4D6fDqHHjxsnX11dr165Vjx49NHz4cHNeXFycevbs6dICAQAAAAAA4D2cDqNsNpvGjBmT5bwFCxbkuSAAAAAAAAB4L6efGXXzzTdr69atWc6Li4vTzTffnOeiAAAAAAAA4J2cDqMOHDiglJSULOddunRJBw8ezHNRAAAAAAAA8E5Oh1HS1Vv1srJv3z4FBATkqSAAAAAAAAB4rxw9M+rTTz/Vp59+av4+ePBgBQYG2rVJTk7W1q1b1aZNG9dWCAAAAAAAAK+RozDq4sWLOnXqlKSro6LOnj2b6VY9Pz8/3X///ZowYYLrqwQAAAAAAIBXyFEYNXjwYA0ePFiSFBERoe+++07169fP18IAAAAAAADgfXIURl1r//79+VEHAAAAAAAACgGnH2C+bds2rVy50vz9/PnzGjJkiJo1a6aXXnpJhmG4tEAAAAAAAAB4D6fDqJEjR+qnn34yfx83bpxmzpypy5cva/LkyZoxY4ZLCwQAAAAAAID3cDqMiouLU/PmzSVJhmFozpw5mjBhgv744w8999xz+vjjj11eJAAAAAAAALyD02HU2bNnVb58eUnS1q1blZCQoN69e0uS7rjjDu3bt8+1FQIAAAAAAMBrOB1GlStXTocPH5YkLVu2TCEhIapevbok6fLlyzwzCgAAAAAAAA45/W16rVq1UkxMjE6fPq033nhDd999tzlv7969Cg8Pd2mBAAAAAAAA8B5Oj4yaPHmybDabnn76afn5+emll14y533zzTdq1qyZSwsEAAAAAACA93B6ZFRERIR27dqlM2fOqGzZsnbzZsyYodDQUJcVBwAAAAAAAO/idBiV4fogSpLq1auXp2IAAAAAAADg3Zy+TU+S/v77b/Xr109hYWHy8/NTxYoV1b9/f/3999+urg8AAAAAAABexOmRUbt27VJ0dLQuXbqk22+/XWFhYTp27Ji+/vpr/fTTT1qzZo1q1aqVH7UCAAAAAACggHM6jHr++edVrlw5LV++XJUqVTKnHzlyRLfffrvGjRun7777zqVFAgAAAAAAwDs4fZveihUrNGHCBLsgSpIqVaqkl156ScuWLXNZcQAAAAAAAPAuTodRFy9eVLly5bKcV758eSUnJ+e5KAAAAAAAAHgnp8OoyMhIzZkzJ8t5X375Jc+LAgAAAAAAgENOPzNq+PDheuyxx5SYmKj+/furQoUKOn78uGbPnq0FCxboww8/zI86AQAAAAAA4AWcDqMeffRRnThxQhMnTtTChQslSYZhqESJEnr11Vf1yCOPuLxIAAAAAAAAeAenwyhJGjt2rIYMGaJ169bpn3/+Ubly5RQdHa2goCBX1wcAAAAAAAAvkuMwauvWrXrnnXd08OBBBQcHq2/fvurUqVN+1gYAAAAAAAAvk6Mwav369br99tt16dIlc9qcOXP0wQcfaODAgflWHAAAAAAAALxLjr5N75VXXlHZsmX166+/6sKFC4qNjVWjRo304osv5nd9AAAAAAAA8CI5CqM2bNigmJgYtWvXTiVKlNAtt9yi9957TydOnNCBAwfyuUQAAAAAAAB4ixyFUQkJCapbt67dtKioKBmGobNnz+ZHXQAAAAAAAPBCOQqjDMOQj4+P3bSM39PT011fFQAAAAAAALxSjr9N78svv9Tq1avN39PT02Wz2TRnzhwtX77cnG6z2fTMM8+4tEgAAAAAAAB4hxyHUdOmTcty+htvvGH3O2EUAAAAAAAAHMlRGLV///78rgMAAAAAAACFQI7CqCpVquR3HQAAAACAa7y25bTG3Fre3WUAgMvl6AHmAAAAAAAAgCsQRgEAAAAAAMAyhFEAAAAAAACwDGEUAAAAAAAALEMYBQAAAAAAAMvkKYxKTk7W0aNHlZqa6qp6AAAAAAAA4MVyFUYtW7ZM0dHRCggIUJUqVbRt2zZJ0tChQ/X999+7tEAAAAAAAAB4D6fDqN9++00dOnTQpUuXNGrUKKWnp5vzypcvr1mzZrmyPgAAAAAAAHgRp8Ool156SXfddZe2bNmiiRMn2s2rX7++YmNjXVUbAAAAAAAAvIyvswts2bJF33zzjSTJZrPZzbvpppt08uRJ11QGAAAAAAAAr+P0yChfX19duXIly3knT55UQEBAnosCAAAAAACAd3I6jGrSpIk+//zzLOd9++23io6OznNRAAAAAADP9tqW0+4uAUAB5XQYNWbMGM2bN089evTQggULZLPZtGHDBj311FP69ttvNXr0aKfWt3LlSnXt2lVhYWGy2Wz64Ycf7OYPGDBANpvN7qdZs2Z2bVJSUjRs2DCVL19e/v7+6tatm44cOWLXJiEhQf369VNQUJCCgoLUr18/nT171tndBwAAAAAAQB44HUbdeeed+vTTT7Vq1Sr16tVLhmFo6NCh+uKLLzRr1iy1bNnSqfVduHBB9evX14wZMxy26dSpk44fP27+/Pzzz3bzR4wYoXnz5mnu3LlavXq1zp8/ry5duigtLc1s06dPH8XGxmrRokVatGiRYmNj1a9fP+d2HgAAAAAAAHni9APMJemhhx5Sr169tHbtWp04cULly5dXixYt5O/v7/S6OnfurM6dO9+wjZ+fn0JDQ7Ocl5iYqI8++kiff/657rzzTknS7NmzFR4erqVLl6pjx47auXOnFi1apPXr16tp06aSpJkzZyo6Olq7d+9WZGSk03UDAAAAAADAeU6PjMpQokQJ3XHHHerTp486dOiQqyAqp5YvX67g4GDVrFlTgwYNsvvGvs2bN+vKlSvq0KGDOS0sLExRUVFau3atJGndunUKCgoygyhJatasmYKCgsw2AAAAAAAAyH85Ghm1cuVKp1baunXrXBWTlc6dO+u+++5TlSpVtH//fr344ou6/fbbtXnzZvn5+Sk+Pl7FihVTmTJl7JYLCQlRfHy8JCk+Pl7BwcGZ1h0cHGy2yUpKSopSUlLM35OSkly0VwAAAAAAAIVTjsKotm3bymazSZIMwzD/7ci1z2rKq/vvv9/8d1RUlBo3bqwqVapo4cKF6tmzp8Plrq8zq5qz25fJkydrwoQJuawcAAAAAAAA18tRGLVs2TLz3+fOndNTTz2lyMhI9enTR6GhoYqPj9ecOXO0e/duvf322/lWrCRVqFBBVapU0d69eyVJoaGhunz5shISEuxGR508eVLNmzc325w4cSLTuk6dOqWQkBCH2xo7dqxGjhxp/p6UlKTw8HBX7QoAAAAAAEChk6Mwqk2bNua/hwwZotatW+uzzz6za9O/f3/169dPP/74o7p06eLaKq/xzz//6PDhw6pQoYIkqVGjRipatKiWLFmi3r17S5KOHz+uuLg4TZkyRZIUHR2txMREbdy4UbfddpskacOGDUpMTDQDq6z4+fnJz88v3/YFAAAAAACgsHH6AebffPON+vbtm+W8vn376vvvv3dqfefPn1dsbKxiY2MlSfv371dsbKwOHTqk8+fPa9SoUVq3bp0OHDig5cuXq2vXripfvrx69OghSQoKCtLAgQP17LPP6tdff9WWLVv00EMPqV69eua369WuXVudOnXSoEGDtH79eq1fv16DBg1Sly5d+CY9AAAAAAAAC+VoZNS1Ll68aPdtdtc6ceKELl686NT6Nm3apHbt2pm/Z9wW179/f7377rvavn27PvvsM509e1YVKlRQu3bt9NVXXykgIMBc5o033pCvr6969+6t5ORk3XHHHZo1a5Z8fHzMNnPmzNHw4cPNb93r1q2bZsyY4VStAAAAAAAAyBunw6hWrVpp3LhxuvXWWxUVFWVO3759u1544QW1atXKqfW1bdtWhmE4nL948eJs11G8eHFNnz5d06dPd9imbNmymj17tlO1AQAAAAAAwLWcDqOmTZum1q1bq0GDBqpbt675APMdO3aoXLlymjZtWn7UCQAAAAAAAC/g9DOjIiMjtX37do0aNUolSpTQvn37VKJECf3rX//Stm3beAYTAAAAAAAAHHJ6ZJQkBQcH67XXXnN1LQAAAAAAF3hty2mNubW8u8sAgCw5PTIKAAAAAAAAyC3CKAAAAAAAAFiGMAoAAAAAAACWIYwCAAAAAACAZQijAAAAAAAAYBnCKAAAAAAAAFjGNyeNXn755Ryv0Gaz6cUXX8x1QQAAAAAAAPBeOQqjYmJi7H632WwyDCPTtAyEUQAAAAAAAMhKjm7TS09PN392796tiIgIvfrqq9q/f7+Sk5O1f/9+vfLKK4qIiNCuXbvyu2YAAAAAAAAUUDkaGXWtp59+Wg8//LDGjh1rTqtSpYqef/55XblyRcOHD9d///tflxYJAAAAAAAA7+D0A8xXrVqlFi1aZDmvRYsWWr16dZ6LAgAAAAAAgHdyOozy8/PTpk2bspy3adMmFStWLM9FAQAAAAAAwDs5fZtejx49NGHCBJUqVUp9+vRRmTJllJCQoDlz5ujll19W375986NOAAAAAAAAeAGnw6ipU6fq77//1rBhwzR8+HD5+voqNTVVhmGodevWmjp1an7UCQAAAAAAAC/gdBgVEBCg3377TYsWLdLy5cv1zz//qFy5cmrXrp06dOggm82WH3UCAAAAAPLBa1tOa8yt5d1dBoBCxOkwKkOnTp3UqVMnV9YCAAAAAAAAL+f0A8wBAAAAAACA3MpVGDV79mw1btxY/v7+8vHxyfQDAAAAAAAAZMXpMGrBggV65JFHdOuttyo5OVmPPPKIHnzwQfn7+6tGjRp66aWX8qNOAAAAAAAAeAGnw6jXXntNI0eO1HvvvSdJGjJkiGbPnq09e/YoLS1N4eHhLi8SAAAAAAAA3sHpMGr37t268847zW/NS01NlSSFhobqhRde0NSpU11bIQAAAAAAALyG02FUWlqaihUrpiJFisjf31/x8fHmvMqVK2vfvn0uLRAAAAAAAADew+kwKiIiQseOHZMk1a9fX19++aU579tvv1WFChVcVx0AAAAAIN+9tuW0u0sAUIg4HUbdcccdWrp0qSTp6aef1ldffaXq1aurTp06eu+99/Tkk0+6vEgAAAAAAAB4B19nF3j11VeVkpIiSbrvvvvk4+OjOXPmyGazafTo0RowYICrawQAAAAAAICXcDqM8vPzk5+fn/l7z5491bNnT5cWBQAAAAAAAO/k9G16AAAAAAAAQG7laGTU7bffnuMV2mw2/frrr7kuCAAAAAAAAN4rR2FUenq6bDab+fvu3bsVHx+vKlWqKDQ0VPHx8Tp48KAqVKigyMjIfCsWAAAAAAAABVuOwqjly5eb/160aJEGDRqkNWvWKDo62py+du1a3X///Ro1apTLiwQAAAAAAIB3cPqZUS+88IJiYmLsgihJat68ucaPH69x48a5rDgAAAAAAAB4F6fDqB07dig8PDzLeZUrV9auXbvyXBQAAAAAAAC8k9NhVEhIiL777rss533zzTcKCQnJc1EAAAAAAADwTjl6ZtS1hgwZojFjxujMmTPq06eP+QDzOXPmaN68eZo8eXJ+1AkAAAAAAAAv4HQYNXr0aF28eFFTpkzR999/L0kyDEPFixfXuHHjNHr0aJcXCQAAAAAAAO/gdBglSTExMXrmmWe0bt06/fPPPypXrpyaNWum0qVLu7g8AAAAAAAAeJNchVGSFBQUpE6dOrmyFgAAAAAAAHi5HIVRhw4dUoUKFVS0aFEdOnQo2/aVK1fOc2EAAAAAAADwPjkKoyIiIrRu3Trddtttqlq1qmw22w3bp6WluaQ4AAAAAAAAeJcchVEff/yxqlWrZv47uzAKAAAAAAAAyEqOwqj+/fub/x4wYEB+1QIAAAAAAAAvV8TZBR599FHt378/y3kHDx7Uo48+mueiAAAAAAAA4J2cDqNmzZqlU6dOZTnv9OnT+vTTT/NcFAAAAAAAALyT02HUjZw5c0Z+fn6uXCUAAAAAAAC8SI6eGbVy5UotX77c/P3DDz/UokWL7NokJydr/vz5qlOnjksLBAAAAAAAgPfIURi1bNkyTZgwQZJks9n04YcfZtmuSpUqevvtt11XHQAAAAAAALxKjsKo0aNH66mnnpJhGAoODtbixYvVsGFDuzZ+fn4qVapUvhQJAAAAAAAA75CjMKpEiRIqUaKEJGn//v2qUKGCihUrlq+FAQAAAAAAwPvkKIy6VpUqVfKjDgAAAAAAABQCTn+b3pUrVzRx4kTVqVNH/v7+8vHxsfvx9XU63wIAAAAAAEAh4XRyNHbsWL3xxhvq3LmzunfvLj8/v/yoCwAAAACQT17bclpjbi3v7jLyjbfvH1DQOR1Gff3113rppZc0fvz4/KgHAAAAAAAAXszp2/QSEhLUunXr/KgFAAAAAAAAXs7pMKp169aKjY3Nh1IAAAAAAPnttS2n3V0CgELO6TDqrbfe0kcffaTvv/9ely9fzo+aAAAAAAAA4KWcfmZUgwYNdOXKFd13332y2WwqWbKk3XybzabExESXFQgAAAAAAADv4XQY1atXL9lstvyoBQAAAAAAAF7O6TBq1qxZ+VAGAAAAAAAACgOnnxkFAAAAAAAA5JbTI6MyxMXFaefOnUpOTs407+GHH85TUQAAAAAAAPBOTodRFy9eVLdu3fTbb7/JZrPJMAxJsnuOFGEUAAAAAAAAsuL0bXqvvPKKDhw4oBUrVsgwDH3//fdasmSJevbsqRo1auiPP/7IjzoBAAAAAADgBZwOo+bPn6/nnntOzZs3lyRVrlxZd9xxh7755hs1bNhQ7777rsuLBAAAAAAAgHdwOow6cOCAatWqJR8fH9lsNl28eNGc17dvX/3www+urA8AAAAAAABexOkwqnTp0rpw4YIkKTg4WHv37jXnXblyxZwHAAAAAAAAXM/pMKpevXras2ePJKldu3aaNGmSVq9erY0bN+rll19W/fr1XV4kAAAAAAAAvIPT36Y3cOBAczTUq6++qpYtW6pNmzaSro6a+vnnn11bIQAAAAAAALyG02FU7969zX9HRERoz549+u2332Sz2dS8eXOVLVvWpQUCAAAAAGC117ac1phby7u7DMArOR1GXc/f319du3Z1RS0AAAAAAADwcjl6ZlRCQoJ69eqln376yWGbn376Sb169dI///zjsuIAAAAAAADgXXIURn344YfaunWrOnXq5LBNp06dtH37dr399ttOFbBy5Up17dpVYWFhstls+uGHH+zmG4ahmJgYhYWFqUSJEmrbtq127Nhh1yYlJUXDhg1T+fLl5e/vr27duunIkSN2bRISEtSvXz8FBQUpKChI/fr109mzZ52qFQAAAAAAAHmTozBq7ty5GjRokHx9Hd/V5+vrq0GDBmnBggVOFXDhwgXVr19fM2bMyHL+lClTNHXqVM2YMUO///67QkND1b59e507d85sM2LECM2bN09z587V6tWrdf78eXXp0kVpaWlmmz59+ig2NlaLFi3SokWLFBsbq379+jlVKwAAAAAABdVrW067uwRAUg6fGbVnzx41btw423YNGzbUK6+84lQBnTt3VufOnbOcZxiG3nzzTY0bN049e/aUJH366acKCQnRF198oSeeeEKJiYn66KOP9Pnnn+vOO++UJM2ePVvh4eFaunSpOnbsqJ07d2rRokVav369mjZtKkmaOXOmoqOjtXv3bkVGRjpVMwAAAAAAAHInRyOjUlNTVbRo0WzbFS1aVFeuXMlzURn279+v+Ph4dejQwZzm5+enNm3aaO3atZKkzZs368qVK3ZtwsLCFBUVZbZZt26dgoKCzCBKkpo1a6agoCCzDQAAAAAAAPJfjsKoChUq6M8//8y23Y4dOxQaGprnojLEx8dLkkJCQuymh4SEmPPi4+NVrFgxlSlT5oZtgoODM60/ODjYbJOVlJQUJSUl2f0AAAAAAAAg93IURrVp00bvvPPODUc9XblyRe+++67atWvnsuIy2Gw2u98Nw8g07XrXt8mqfXbrmTx5svnA86CgIIWHhztZOQAAAAAAAK6VozDqmWee0a5du9SjRw8dO3Ys0/xjx46pe/fu2r17t5555hmXFZcxyur60UsnT540R0uFhobq8uXLSkhIuGGbEydOZFr/qVOnMo26utbYsWOVmJho/hw+fDhP+wMAAAAAAFDY5SiMuuWWW/T2229r8eLFioiIUPPmzdW3b1/17dtXzZs3V0REhH755Re9/fbbqlevnsuKi4iIUGhoqJYsWWJOu3z5slasWKHmzZtLkho1aqSiRYvatTl+/Lji4uLMNtHR0UpMTNTGjRvNNhs2bFBiYqLZJit+fn4KDAy0+wEAAAAAAEDu5ejb9CRp0KBBioqK0qRJk7Rs2TKtX79eklSyZEl16tRJY8eOVbNmzZwu4Pz58/rrr7/M3/fv36/Y2FiVLVtWlStX1ogRIzRp0iTVqFFDNWrU0KRJk1SyZEn16dNHkhQUFKSBAwfq2WefVbly5VS2bFmNGjVK9erVM79dr3bt2urUqZMGDRqk999/X5L0+OOPq0uXLnyTHgAAAAAAgIVyHEZJV0cY/fjjj0pPT9fp06clSeXLl1eRIjkaYJWlTZs22T1nauTIkZKk/v37a9asWRo9erSSk5M1ZMgQJSQkqGnTpvrll18UEBBgLvPGG2/I19dXvXv3VnJysu644w7NmjVLPj4+Zps5c+Zo+PDh5rfudevWTTNmzMh13QAAAAAAAHCeU2FUhiJFimT57XS50bZtWxmG4XC+zWZTTEyMYmJiHLYpXry4pk+frunTpztsU7ZsWc2ePTsvpQIAAAAAACCPcj+kCQAAAAAAAHASYRQAAAAAAAAsQxgFAAAAAAAAyxBGAQAAAAAAwDKEUQAAAAAAALAMYRQAAAAAAAAsQxgFAAAAAAAAyxBGAQAAAAAAwDKEUQAAAAAAABZ7bctpd5fgNoRRAAAAAFDAFeaLWgAFD2EUAAAAAAAALEMYBQAAAABANhh9BrgOYRQAAAAAAAAsQxgFAAAAAAAAyxBGAQAAAAAAwDKEUQAAAAAAALAMYRQAAAAAAAAsQxgFAAAAAIAT+GY9IG8IowAAAAAAAGAZwigAAAAAAPIRI6kAe4RRAAAAAAAAsAxhFAAAAAAAACxDGAUAAAAAQA5xyx2Qd4RRAAAAAADkg9wEV4RdKAwIowAAAAAAAGAZwigAAAAAAABYhjAKAAAAAAoQbuMCUNARRgEAAABAAUAIhcKCc937EUYBAAAAAOAAwQjgeoRRAAAAAAAAsAxhFAAAAAAAACxDGAUAAAAAQAHFbYQoiAijAAAAAABAgUD45h0IowAAAAAAAGAZwigAAAAAQKFm9WgbRvegsCOMAgAAAAAAgGUIowAAAAAAAGAZwigAAAAAQK5wu1n2bnSMOH4orAijAAAAAAAAYBnCKAAAAACAJEbqALAGYRQAAAAAAAAsQxgFAAAAAAAAyxBGAQAAAIAX4pY7AJ6KMAoAAAAACjBCJwAFDWEUAAAAAAAALEMYBQAAAAAAAMsQRgEAAAAAPBq3IgLehTAKAAAAAJAjVoVC7g6f3L19wNsRRgEAAAAAAMAyhFEAAAAAAACwDGEUAAAAAAAALEMYBQAAAADINzx/CcD1CKMAAAAAAHAhTwjgPKEGwBHCKAAAAAAALEBABFxFGAUAAAAAgAcgrEJhQRgFAAAAAAAAyxBGAQAAAAAKNEYUAQULYRQAAAAAeAECGQAFBWEUAAAAAAAALEMYBQAAAAAAAMsQRgEAAAAAAMAyhFEAAAAAAACwDGEUAAAAAAAeigfTwxsRRgEAAAAAvBqBDuBZCKMAAAAAAPAChG4oKAijAAAAAAAAYBnCKAAAAAAAAFiGMAoAAAAAAItwKx1AGAUAAAAAQKFDKAZ3IowCAAAAAACAZQijAAAAAAAAYBnCKAAAAAAAciC/bm3jljkUNoRRAAAAAADkEkES4DyPD6NiYmJks9nsfkJDQ835hmEoJiZGYWFhKlGihNq2basdO3bYrSMlJUXDhg1T+fLl5e/vr27duunIkSNW7woAAAAA4P8jxAEKL48PoySpbt26On78uPmzfft2c96UKVM0depUzZgxQ7///rtCQ0PVvn17nTt3zmwzYsQIzZs3T3PnztXq1at1/vx5denSRWlpae7YHQAAAAAAgELL190F5ISvr6/daKgMhmHozTff1Lhx49SzZ09J0qeffqqQkBB98cUXeuKJJ5SYmKiPPvpIn3/+ue68805J0uzZsxUeHq6lS5eqY8eOlu4LAAAAADiLUUQAvEmBGBm1d+9ehYWFKSIiQg888ID27dsnSdq/f7/i4+PVoUMHs62fn5/atGmjtWvXSpI2b96sK1eu2LUJCwtTVFSU2QYAAAAA8D/eHH7lx7558/EC8oPHj4xq2rSpPvvsM9WsWVMnTpzQxIkT1bx5c+3YsUPx8fGSpJCQELtlQkJCdPDgQUlSfHy8ihUrpjJlymRqk7G8IykpKUpJSTF/T0pKcsUuAQAAAAAAFFoeH0Z17tzZ/He9evUUHR2tatWq6dNPP1WzZs0kSTabzW4ZwzAyTbteTtpMnjxZEyZMyGXlAAAAAAAAuF6BuE3vWv7+/qpXr5727t1rPkfq+hFOJ0+eNEdLhYaG6vLly0pISHDYxpGxY8cqMTHR/Dl8+LAL9wQAAAAAkFPcCgd4jwIXRqWkpGjnzp2qUKGCIiIiFBoaqiVLlpjzL1++rBUrVqh58+aSpEaNGqlo0aJ2bY4fP664uDizjSN+fn4KDAy0+wEAAACAgopAB56I87Lw8fjb9EaNGqWuXbuqcuXKOnnypCZOnKikpCT1799fNptNI0aM0KRJk1SjRg3VqFFDkyZNUsmSJdWnTx9JUlBQkAYOHKhnn31W5cqVU9myZTVq1CjVq1fP/HY9AAAAAAA8DSENvJXHh1FHjhzRgw8+qNOnT+umm25Ss2bNtH79elWpUkWSNHr0aCUnJ2vIkCFKSEhQ06ZN9csvvyggIMBcxxtvvCFfX1/17t1bycnJuuOOOzRr1iz5+Pi4a7cAAAAAAAAKJY8Po+bOnXvD+TabTTExMYqJiXHYpnjx4po+fbqmT5/u4uoAAAAAAADgjAL3zCgAAAAAAAAUXIRRAACgwOEZGgA8kae/N3l6fQAKD8IoAAAAAEC+KAgBWEGo0RUKy36iYCCMAgAAAADkGKGGZ+J1QUFCGAUAAFDAcMEBAK6X1Xsr77dA/iCMAgAAAAAAgGUIowAAAADADQrCqJuCUCOAgocwCgAAAADgMgRYALLj6+4CAAAAAACAcwj9UJAxMgoAAAAAALgV4VrhQhgFAAAAAAAAyxBGAQAAAICHKkijRTy9Vk+vL6e8ZT9QuBFGAQAAAAAc8tTww1PrApA9wigAAAAAALwEIR0KAsIoAAAAAHAjwgMAhQ1hFAAAAAAAsBQhbOFGGAUAAAAAQCFEIAR3IYwCAAAAAACAZQijAAAAAAAeh1E79jge8CaEUQAAAAAAIBMCMOQXwigAAAAAgNchSLmK4wBPRBgFAAAAABYjIIC34ZyGMwijAAAAgEKAC0UAVuI9BzdCGAUAAAAAOcDFtefjNbIWxxu5RRgFAAByjA+dAJD/eK91jGOTPzKOq6uOL68TskMYBQAAAAAW4AId3spTz21PrQuEUQDgtfjjCwAAAMATEUYBAAAAQAHB/2y6iuMAFGyEUQAAAAAKFU8MMjyxptzwlv3IicK0r4CrEUYBAAAAAGCxghhmWVmzu49PXrfv7vo9HWEUAAAAABRQXPAiv3jruXXtfnnrPhYEhFEAAAAA4GG4SAb+Jy/9gb7kmQijAAAAAOQ7LggBeCLem9yDMAoAAAAAwEU5cAOu6B/0sf8hjAIAAIBb8KHcc3n6a+NJ9XlSLQBQUBBGAQAAAF6EcAQA4OkIowAAAACgECK4dD2OqWfK6evC62cdwigAAAAAAIBcIMDKHcIoAAAAoJDgogmwjrP9zdP7p6vq87bjgtwhjAIAAAAKuewu9lx5MciFJbwV53b+4xh7D8IoAABQqPBBFoWNVec8fcsexyN3GDWTP/J6nFxxnK9dB68bCKMAACjAvOHDnDfsAwBkh/c6oOCgv+Y/wigAAAAATvPUizVPrQv/40mvkSfVYqXCut85kV/HhmNujzAKAADAAnwIRUFQGM7TwrCPcIzXH/AMhFEAAAAA4CKEHY5Z+aB8eI78eF05Vwo+wigAAGApPkACBUdu+2thCR28ZT/ciWMIFE6EUQAAAADcxpXfnuZoHoGH63nzMfXmfYNrufpbBgsTwigA8EKF9Y+at+L1BHAjvEdwDAA4J6v3DN5HrEUYBQAAAHiB/L6Qyq9b9gC4Hv0u/3GM84YwCgCAfMaHFcC7vbbldKHv5+7a/8J+3PMbxxcFRX6eq/SD/EEYBQBegj+U8CScjwAAFFwF5e94QakTmRFGAQCAXOEDIICCwJn3Kle/r3nT+6Q37Qvs8drmXGH5plArEEYBAIAcyc03WAG4ij4Cb8c5nr8K0vF1Z63uDJ/hHMIoAAC8hLufl8CHOuQV51Dh5Umvvbtrcff2vQHHsGDxthGJ7t5+QUEYBQAAAFiICxXvwLcLFgwcb7gT559jhFEA4AX4QwdvxHmNvOD88UyufF285TX2lv0AAGcQRgHAdfhQCFey+nzi/PUcvBZAwUBfBTzH9f2R/um9CKMAALBAfnyYKigf0ApKne7EMYInyuuz4jzhvPaEGnCVu59r6G3ceTwL4/GG6xFGoUDjjRAA4K3y+jeOv5GuU9iOZWHa38K0r56GY4/CinP/KsIoAAAKGG/8EOON+4Sc8bbX3tv253revn8AkBu8NzqPMAoAAC/DByL34lYUXM9TX7fXtpz26No8zbU1eWJ93ozj7T0K6pcYcA66HmEUAI/Hmz9wY/QRwPMUpOfEFYT3kLw+vwqejYdWF2ye/uw4T8BxyIwwCvAAvDkhv3Bu4Xru/MBvxbY456/iwu6qgvDcLUKW7Ll7/929fQDwRoRRAFCAcRHjfQra61XQ6vVkhXnUizfj+OdcfoeoVoa0vO6A5ymotwh6K8IoAC7BGzLg+einyFCQz4WCXDusxbkCAJ6LMAoAvIgzH7z5kI6suPMBvXnZXnbLWrUvBbVfFdS6XcmZc99Tjpen1HG9/OzLAADvQBiFAosPK4Wbp97Okt9D/l21fvqP5+E1cZ2CEsq6+nYBT3j/4jzOXsYxcnSseN5XwcTrBLgf/bBgIYwCgEKgoHwdtSfV6QkX9q5cjzcqiMfY1SNG3HV+OLvd7AIYT1FQgszCxFOOs6fUAQDegjAKBU5BuWiAa3nia+GJNbmSJ+6fp9TkKXXkhLtrLWyBUUFSUG5F8yQF4XZPK4I/Rm8BwI3xvpg9wiigAMjt/4F2dx24sew+zN/otpv8/IYhT32dPbUuq3jLM1hyeqGc39944ynHxFPqyE5e3osKwj4WtNA0N9spCK+Du/HtegBgHcIowMMUlFsu8rqcq9eRX/K7NqvCprzypOfqXBtmWHVLTW4vuJ0NQZwZ0eCKMCe3IaSnnZ/5yRPfk61+rZw91/IroMrpMq46dt7Ok4MzK9fn7u0AQGFFGAWv5S0fIgrSflx/YevsBYon7auji/S8XnR50j5ezxUXh9kFRLkNbDz5uOWGs0HatctZxaqg2dNG1hTEIMNT68qOVX8TPClMz491FdTXHwBQuBFGAQ648/88e3Jo4wkXB9ePHsnJsXNHuOEtFwh5OXaeFrjkNSjzltdUsuYiOT/W7S23K96Iu+t05ag7V78HuPqWy4IYPgIA4A0KXRj1zjvvKCIiQsWLF1ejRo20atUqd5fkFQrCB7ZrRyLktV5PvTUtJ6NScht6WRVA5PYWrNxyxaglZ0cHuZOn1ZWfgYgzowhuFHC6arvu5sxFtye8R7piu67qizkdKZldLTltm9N5ueWq7bl7xJqz2/eE/5niSp5SBwAAuVGowqivvvpKI0aM0Lhx47Rlyxa1atVKnTt31qFDh9xdmkfx5A83N7q4zM0H1bwEH9ld6F5/wZKTbd3o4sXKICknoZaz63VUm6va5veFdk6PSXbryO2yN2pv1aiUvJ5Tjtbr7PqyW39OgmdXBNOuCDKRmTuPZW7eR6wIn/MztPUUrvh74szfC085Ds78LQcAwJsUqjBq6tSpGjhwoB577DHVrl1bb775psLDw/Xuu++6uzTLWf3hJqcXflldRDp70ZiXD5l5GRWRm6DBFaMPnAncXLnt3MrPEReOgkBHy7oqiHBFe3cGG67atrPr8ZSLLFe+lvl5XuB/rDgu+TnqpiC9rrn9nyC5mZ8X+fl+btXr5UkhGQAA+a3QhFGXL1/W5s2b1aFDB7vpHTp00Nq1a91UlXvdaNTNtfMdjTpwtExORijcaB152Y/8WD6741TQuSI0yMvFuadenOQnT63LWfl5se5JCmrdyJ43vrbesk/u/mwAAADyl6+7C7DK6dOnlZaWppCQELvpISEhio+Pz3KZlJQUpaSkmL8nJiZKkpKSkvKvUItcOn9OkpSUVExTt/5jTo9Z9b/pl86fM3/PmDeyfjlz2QzOtL22/fXTrq3rRnU4WseN1n2jfcxNfY7a5qZuZ+rIa305rdvT6stu3dRHfdRHfdRHfdRHfdRHfdRHfQW1vozfC7KMnMQwjBy1txk5bVnAHTt2TBUrVtTatWsVHR1tTn/11Vf1+eefa9euXZmWiYmJ0YQJE6wsEwAAAAAAoEA6fPiwKlWqlG27QjMyqnz58vLx8ck0CurkyZOZRktlGDt2rEaOHGn+np6erjNnzqhcuXKy2Wz5Wq+3S0pKUnh4uA4fPqzAwEB3lwN4FPoH4Bj9A3CM/gHcGH0EcCyv/cMwDJ07d05hYWE5al9owqhixYqpUaNGWrJkiXr06GFOX7Jkie65554sl/Hz85Ofn5/dtNKlS+dnmYVOYGAgfwgAB+gfgGP0D8Ax+gdwY/QRwLG89I+goKActy00YZQkjRw5Uv369VPjxo0VHR2tDz74QIcOHdKTTz7p7tIAAAAAAAAKhUIVRt1///36559/9PLLL+v48eOKiorSzz//rCpVqri7NAAAAAAAgEKhUIVRkjRkyBANGTLE3WUUen5+fho/fnym2yAB0D+AG6F/AI7RP4Abo48AjlndPwrNt+kBAAAAAADA/Yq4uwAAAAAAAAAUHoRRAAAAAAAAsAxhFAAAAAAAACxDGAUAAAAAAADLEEbBIyUkJCg5OdndZQAACiC+mwVwjP4BAMgNV//9IIyCx9mxY4fq1Kmjn3/+2d2lAB7n1KlT2rZtm7Zt2+buUgCPc/HiRUnSuXPn3FwJ4HkuXLigtLQ0+gdwA4S1QGZXrlyRJF26dEmSlJ6e7pL1EkbBo8TGxqply5ZKSkrSe++9pzNnzri7JMBjbN++XW3btlXfvn3VoEEDxcTEuLskwGPExcWpV69euv3229W2bVt9+OGHOnXqlLvLAjxCXFycunXrpujoaDVv3lwffPCBTpw44e6yAI+wZ88e/fjjj5Ikm81GIAVcY9euXRo8eLDat2+v/v37a+PGjSpSpIhL+glhFDzG1q1b1bx5cz311FP6+OOPtX37dh0/flyS69JXoKD666+/1L59e/Xo0UPffPONPv74Y7388ss6cuSIu0sD3G7Pnj1q166d6tatq379+ql79+56/PHHNWrUKP3+++/uLg9wq3379ql169aKiorSww8/rO7du2v48OEaPXo0/QOF3t69e9WkSRPdc889+vzzzyURSAEZ4uLi1KJFCxUtWlSRkZFKS0tT//79tX//ftlstjyv39cFNQJ5tmXLFjVq1EjPP/+8XnnlFUnSq6++qvHjx+vbb79VkSLkpijcZs2apSZNmmjixImSpEqVKumbb77RyZMndejQIVWvXl3BwcFurhJwj3feeUcdOnTQ66+/bk7bs2eP5s6dK0kaO3asatWq5a7yALf64YcfVKdOHU2bNs2c1rp1aw0bNkypqakaM2aM6tWr58YKAfc4c+aMxowZo7Zt26pKlSoaOnSo0tLSNGDAADOQcsUFN1AQxcfH69FHH9XAgQM1ZcoUSdIff/yhAQMG6M8//1RERESe+whX+HC7tLQ0ffvtt/rXv/6liRMnKi0tTZL02GOPac+ePdq6dask7uFG4Xb06FEVKVLEvGf7rbfe0uLFi/Xkk0+qU6dOeuKJJ7Rx40Y3VwlYzzAM/fXXXypXrpyk/z03KjIyUp07d9b8+fP1xRdfmG2BwubChQu6fPmy0tPTlZaWprS0NHXo0EEzZszQ8uXLNWvWLEn0DxQ+iYmJKl26tJ588kk999xzGjJkiIYPH272CUZIoTDbtWuXSpUqpT59+pj9oGHDhgoKClJsbKxLtkEYBbfz8fHR888/r3//+9+SZI6C6tatm44ePap58+ZJEv9nAoVaq1at9NNPP+nRRx/VQw89pAkTJui7777T0qVLtXLlSv35559auHChu8sELGez2RQVFaWffvpJx48fV8mSJXX06FG9/vrrevbZZzV16lRNnTpVhw4d4u8ICqXatWvrjz/+0B9//CEfHx8ZhiHDMNS+fXu9+eabevPNN7V+/Xr6BwqdiIgIvfDCC+rcubMqVqyooUOHavDgwZkCqdTUVPPBzUBhUaVKFQ0ePFgNGjQw+4EklSxZ0vyf49f+3cjNY3UIo+BWGSetv7+/Oc1msyk9PV1Vq1bVv/71L3322WfauXOnu0oE3CbjgkGSHn30Uc2cOVPVq1dXcnKyHnvsMd1zzz0qVaqUGjRooObNm2vVqlXmHwrA2137f6vvvfde1axZUzVr1tQ999yjmjVr6sEHH1SrVq3Upk0bBQYG6vTp026sFnCf7t27q1evXurbt6927dolX19f80Kie/fuqlWrljZv3uzmKgH3qFKlivnv8PBwDR8+PFMgNXLkSM2cOZNn2KJQiYiI0L333ivp6jW7r+/VJzyVLl3a/BsiSRMmTNCGDRty9VgdnhkFtzhx4oRCQkLMJ/Ff/3/jMk7m6OhovfXWW9q+fbtq166t9PR0nh8Fr5fRPzKC2Yz+8eijj5r/vemmmyTJ7BMpKSmqW7cu/QNe7/r+UaRIETVu3Fhvv/225s+fr3Pnzum+++7TQw89JElKSkpS6dKlVbJkSTdXDuS/AwcOaP78+UpISFD16tX10EMPydfXV0OGDNGkSZP00EMPafbs2eYz1Gw2m0qUKKESJUq4uXIg/2XVP66/FqlYsaKGDx8u6WoI9cknn2jVqlXavHkzn7Hg1a7tH9WqVVO/fv3sPmtdK+OxOi+++KJeffVVde3aNVfbJIyC5Xbu3Km6deuqS5cuWrBgwQ0fENimTRu1b99ezz//vLp27cqHJXi96/tHkSJFMv0RqFmzpl5++WV17NhRfn5+mj9/vpYsWaKVK1fyQQleLav+kZqaKl9fX1WrVk0jR47MtMycOXNUokQJHvAPr7d9+3Z17txZtWvXVmJiorZt26Z9+/bppZdeUps2bZSSkqI333xTzZs31+uvv67AwEBt3rxZ+/fvV9u2bd1dPpCvsuof+/fv14svvpjpGqRixYp68skntWDBAsXFxSk2Nla33HKLmyoH8l9W/ePgwYN64YUXzGuLjOuR8+fPKzAwUNOnT9f//d//adOmTWrYsGHuNmwAFjp+/LjRokULo02bNkZoaKjRvXt3c156erpd27S0NMMwDOPrr782mjRpYsTHx1taK2C17PpHRh85dOiQ8eCDDxo2m82oXbu2Ua9ePWPLli1uqhqwxo36R8bfi2stXbrUGDx4sBEYGEj/gNc7cOCAUa1aNWP06NFGenq6kZSUZLz//vtGnTp1jL1795rt/vrrL2P06NFGWFiYUadOHaNJkybGH3/84cbKgfx3o/6xb9++TO3T0tKMUaNGGb6+vsa2bdvcUDFgHWf7R58+fQwfHx8jICDA2LhxY562zcgoWGrDhg0KDw/XkCFDlJqaqgceeEA9evTQvHnzMg0DvPZB5q1atVJISIg7SwfyXXb9Iy0tTT4+PgoPD9cXX3yhwYMHKzAwUBUqVGDUB7zejfpHViMIAwICdP78ea1du1Z169Z1Y+VA/kpPT9dXX32lGjVqaNy4cbLZbAoICFCjRo106tQpu2d7VKtWTf/+9781bNgwlSpVStLV538A3iq7/pHVg8mPHTumo0eP6vfff1e9evXcUDVgjdz0j5tuukklS5bU2rVrFRUVlaftE0bBUm3atJGfn59atWolSZo7d64eeOABde/eXT/88EOWz5Dy8/NTaGiou0oGLJNd//Dx8VFaWpqKFCkim81mtgMKg5z+/ZCuPgfntttu08yZM+Xn5+fOsoF8l/HctPT0dAUGBkq6+oD/W265RQEBAUpISMi0TFhYGLd1o1DITf+oVKmSPv74YxUvXtzqcgFL5aZ/DBgwQKNGjVKlSpXyvv08rwFwQunSpdWpUyfz97Zt2+qrr77SunXr1L17d0lXLyLef/99rVu3zk1VAu6Rk/7h4+OjDz74gP6BQienfz8++OADrV27VpJUrFgxd5QKWK5Vq1YaM2aMJJn/U69o0aKy2WxKTk422y1dupQvg0Gh40z/yPhWYoIoFBY57R9LliyRJDVo0MAlQZTEyCjks0OHDmn79u06fvy47r77bgUFBalkyZLmByGbzabWrVvrq6++0v3336+ePXsqLCxM77zzjv766y93lw/kK/oH4Jgr+kdWX4wBeIOM/nHs2DF16dJFgYGBKlq0qHk7d2pqqlJSUpSammp++csLL7ygSZMm6ciRIwoLC3PzHgD5h/4BOJaX/nH48GFVrFjRZbXYjIwx7YCLbdu2TR06dFBYWJj279+vgIAA3X///RoyZIgiIiIy/Z+5pUuXqkOHDipTpox++eUXNWrUyI3VA/mL/gE4Rv8AHMuufxiGobS0NF2+fFl16tTRDz/8oP/+97+aNGmSli1bpsaNG7t7F4B8Q/8AHPO0/sEYXeSLs2fP6tFHH9XDDz+sX3/9VQkJCXrssce0YcMGjRgxQn/99Zfd8z3S09P19ddfq2TJklq1ahUXEvBq9A/AMfoH4FhO+ofNZpOvr69KliypcuXK6fHHH1dMTAwX2vB69A/AMU/sH4RRyBdJSUk6ffq07rzzTpUpU0aS9NJLL+mxxx7T2bNnNX78eB0/fty8hWLVqlXasGGDli9frjp16rizdCDf0T8Ax+gfgGM56R/x8fGSpISEBP3999/asmWLNm3axIU2vB79A3DME/sHYRTyhY+Pj0qUKKFjx45JkvkwwIcfflh9+/ZVXFyc+RA0SWrUqJGWLl3KHwIUCvQPwDH6B+BYTvrHL7/8IkkqU6aM3n77bW3fvp2vp0ehQP8AHPPE/sEzo5BvunXrpsOHD2vZsmUqXbq0UlNT5et79Zn59913n44ePaq1a9eaT+0HChP6B+AY/QNwLKf9QxLfnIdCh/4BOOZp/YPeB5e4cOGCzp07p6SkJHPaxx9/rMTERPXu3VuXL182T3RJ6tixowzD0OXLl7mQgNejfwCO0T8Ax3LbP1JSUiSJC214NfoH4FhB6B/0QOTZn3/+qZ49e6pNmzaqXbu25syZo/T0dJUvX15ffPGFdu3apQ4dOmj37t26dOmSJGnjxo0KCAgQA/Pg7egfgGP0D8CxvPQPwNvRPwDHCkr/4DY95Mmff/6p1q1b6+GHH1aTJk20adMmTZ8+XRs2bNCtt94qSYqLi1OfPn108eJFlSlTRhUqVNDy5cu1atUq1a9f3817AOQf+gfgGP0DcIz+AThG/wAcK0j9gzAKuXbmzBk9+OCDqlWrlqZNm2ZOv/3221WvXj1NmzbN7nkeb7/9to4cOaISJUro/vvvV2RkpLtKB/Id/QNwjP4BOEb/AByjfwCOFbT+4Zt9EyBrV65c0dmzZ3XvvfdK+t9Dzm6++Wb9888/kiSbzaa0tDT5+Pho6NCh7iwXsBT9A3CM/gE4Rv8AHKN/AI4VtP7BM6OQayEhIZo9e7ZatWolSUpLS5MkVaxY0e6BZz4+Pjp37pz5O4PxUBjQPwDH6B+AY/QPwDH6B+BYQesfhFHIkxo1aki6mroWLVpU0tWT/sSJE2abyZMna+bMmUpNTZUkvv0IhQb9A3CM/gE4Rv8AHKN/AI4VpP7BbXpwiSJFipj3n9psNvn4+EiSXnrpJU2cOFFbtmyx++pIoDChfwCO0T8Ax+gfgGP0D8CxgtA/GBkFl8kY3ufj46Pw8HC9/vrrmjJlijZt2sS3VqDQo38AjtE/AMfoH4Bj9A/AMU/vH0TFcJmM+1CLFi2qmTNnKjAwUKtXr1bDhg3dXBngfvQPwDH6B+AY/QNwjP4BOObp/YORUXC5jh07SpLWrl2rxo0bu7kawLPQPwDH6B+AY/QPwDH6B+CYp/YPm8FXCyAfXLhwQf7+/u4uA/BI9A/AMfoH4Bj9A3CM/gE45on9gzAKAAAAAAAAluE2PQAAAAAAAFiGMAoAAAAAAACWIYwCAAAAAACAZQijAAAAAAAAYBnCKAAAAAAAAFiGMAoAAAAAAACWIYwCAADIR7NmzZLNZsvyZ9SoUS7dVkxMjGw2m06fPu3S9QIAALiSr7sLAAAAKAw++eQT1apVy25aWFiYm6oBAABwH8IoAAAAC0RFRalx48buLgMAAMDtuE0PAADAzb766itFR0fL399fpUqVUseOHbVly5ZM7TZs2KCuXbuqXLlyKl68uKpVq6YRI0ZkanfixAk9+OCDCgoKUkhIiB599FElJibatXn77bfVunVrBQcHy9/fX/Xq1dOUKVN05cqV/NpNAAAASYyMAgAAsERaWppSU1Ptpvn6+mrSpEl64YUX9Mgjj+iFF17Q5cuX9X//939q1aqVNm7cqDp16kiSFi9erK5du6p27dqaOnWqKleurAMHDuiXX37JtK1evXrp/vvv18CBA7V9+3aNHTtWkvTxxx+bbf7++2/16dNHERERKlasmLZu3apXX31Vu3btsmsHAADgajbDMAx3FwEAAOCtZs2apUceeSTLeYcOHdLNN9+swYMH66233jKnnz9/XjVq1FDr1q311VdfSZKqV68uSYqLi1Px4sWzXF9MTIwmTJigKVOm6F//+pc5fejQofr444918eJF2Wy2TMulp6crPT1dX375pR555BGdOnVKZcqUyfU+AwAA3AgjowAAACzw2WefqXbt2nbTFi9erNTUVD388MN2o6aKFy+uNm3aaNmyZZKkPXv26O+//9akSZMcBlHX6tatm93vt9xyiy5duqSTJ08qJCREkrRlyxaNHz9ea9as0ZkzZ+za79mzR02bNs3VfgIAAGSHMAoAAMACtWvXzvQA88WLF0uSmjRpkuUyRYpcfbznqVOnJEmVKlXK0bbKlStn97ufn58kKTk5WdLVEVmtWrVSZGSkpk2bpqpVq6p48eLauHGjhg4darYDAADID4RRAAAAblK+fHlJ0rfffqsqVao4bHfTTTdJko4cOeKS7f7www+6cOGCvv/+e7vtxsbGumT9AAAAN0IYBQAA4CYdO3aUr6+v/v77b/Xq1cthu5o1a6patWr6+OOPNXLkSHOkU25lPDfq2vUYhqGZM2fmab0AAAA5QRgFAADgJlWrVtXLL7+scePGad++ferUqZPKlCmjEydOaOPGjfL399eECRMkSW+//ba6du2qZs2a6ZlnnlHlypV16NAhLV68WHPmzHFqu+3bt1exYsX04IMPavTo0bp06ZLeffddJSQk5MduAgAA2Cni7gIAAAAKs7Fjx+rbb7/Vnj171L9/f3Xs2FGjR4/WwYMH1bp1a7Ndx44dtXLlSlWoUEHDhw9Xp06d9PLLL5sPJHdGrVq19N133ykhIUE9e/bUsGHD1KBBA7tv9AMAAMgvNsMwDHcXAQAAAAAAgMKBkVEAAAAAAACwDGEUAAAAAAAALEMYBQAAAAAAAMsQRgEAAAAAAMAyhFEAAAAAAACwDGEUAAAAAAAALEMYBQAAAAAAAMsQRgEAAAAAAMAyhFEAAAAAAACwDGEUAAAAAAAALEMYBQAAAAAAAMsQRgEAAAAAAMAy/w8Q/H+tI82lHAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convertir la columna 'created_utc' a tipo datetime\n",
    "comments['created_utc'] = pd.to_datetime(comments['created_utc'])\n",
    "\n",
    "# Extraer el año y el día de la columna 'created_utc'\n",
    "comments['year'] = comments['created_utc'].dt.year\n",
    "comments['day'] = comments['created_utc'].dt.date\n",
    "\n",
    "# Contar los posts por día\n",
    "posts_per_day = comments.groupby('day').size()\n",
    "\n",
    "# Crear el histograma\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(posts_per_day.index, posts_per_day.values, color='skyblue')\n",
    "\n",
    "# Configurar el gráfico\n",
    "plt.title('Cantidad de Comentarios por Día', fontsize=16)\n",
    "plt.xlabel('Fecha', fontsize=12)\n",
    "plt.ylabel('Cantidad de Posts', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de Sentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (883 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas originales: 297533\n",
      "Filas después del filtrado: 291727\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# Ruta al tokenizador\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('../Sentiment_Model/TFM/results/checkpoint-5358')\n",
    "\n",
    "# Ruta al tokenizador y al modelo\n",
    "best_checkpoint = '../Sentiment_Model/TFM/results/checkpoint-5358'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(best_checkpoint)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(best_checkpoint)\n",
    "\n",
    "# Crear pipeline de clasificación\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=-1)  # Usar CPU si no tienes GPU\n",
    "\n",
    "# Función para contar los tokens de cada comentario\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text, truncation=False))\n",
    "\n",
    "# Crear una nueva columna con la cantidad de tokens por comentario\n",
    "comments['num_tokens'] = comments['body'].apply(count_tokens)\n",
    "\n",
    "# Filtrar los comentarios con 512 tokens o menos\n",
    "filtered_comments = comments[comments['num_tokens'] <= 512]\n",
    "\n",
    "# Eliminar la columna auxiliar si no la necesitas\n",
    "filtered_comments = filtered_comments.drop(columns=['num_tokens'])\n",
    "\n",
    "# Verificar el resultado\n",
    "print(f\"Filas originales: {len(comments)}\")\n",
    "print(f\"Filas después del filtrado: {len(filtered_comments)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Realizar las predicciones en lotes\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predict_in_batches(body, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n",
      "Cell \u001b[1;32mIn[49], line 17\u001b[0m, in \u001b[0;36mpredict_in_batches\u001b[1;34m(comments, batch_size)\u001b[0m\n\u001b[0;32m     14\u001b[0m     batch \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Realizar predicciones\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     batch_predictions \u001b[38;5;241m=\u001b[39m classifier(batch)\n\u001b[0;32m     18\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mextend(batch_predictions)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:159\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    158\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,)\n\u001b[1;32m--> 159\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1343\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[0;32m   1340\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   1341\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[0;32m   1342\u001b[0m     )\n\u001b[1;32m-> 1343\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(final_iterator)\n\u001b[0;32m   1344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[0;32m   1345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1269\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1268\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1269\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1270\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:190\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[1;34m(self, model_inputs)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    189\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:977\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    975\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 977\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistilbert(\n\u001b[0;32m    978\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    979\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    980\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    981\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    982\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    983\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    984\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    985\u001b[0m )\n\u001b[0;32m    986\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[0;32m    987\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:797\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_sdpa \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[0;32m    793\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_attention_mask_for_sdpa(\n\u001b[0;32m    794\u001b[0m             attention_mask, embeddings\u001b[38;5;241m.\u001b[39mdtype, tgt_len\u001b[38;5;241m=\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    795\u001b[0m         )\n\u001b[1;32m--> 797\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m    798\u001b[0m     x\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[0;32m    799\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    800\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    801\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    802\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    803\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    804\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:550\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    542\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    543\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    544\u001b[0m         hidden_state,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         output_attentions,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 550\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    551\u001b[0m         hidden_state,\n\u001b[0;32m    552\u001b[0m         attn_mask,\n\u001b[0;32m    553\u001b[0m         head_mask[i],\n\u001b[0;32m    554\u001b[0m         output_attentions,\n\u001b[0;32m    555\u001b[0m     )\n\u001b[0;32m    557\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:476\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[1;32m--> 476\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m    477\u001b[0m     query\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    478\u001b[0m     key\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    479\u001b[0m     value\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    480\u001b[0m     mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[0;32m    481\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    482\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    483\u001b[0m )\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    485\u001b[0m     sa_output, sa_weights \u001b[38;5;241m=\u001b[39m sa_output  \u001b[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:412\u001b[0m, in \u001b[0;36mDistilBertSdpaAttention.forward\u001b[1;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    402\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[0;32m    403\u001b[0m     q,\n\u001b[0;32m    404\u001b[0m     k,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    408\u001b[0m     is_causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    409\u001b[0m )\n\u001b[0;32m    411\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m unshape(attn_output)\n\u001b[1;32m--> 412\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_lin(attn_output)\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (attn_output,)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Obtener la lista de comentarios filtrados\n",
    "body = filtered_comments['body'].tolist()\n",
    "\n",
    "# Función para predecir en lotes\n",
    "def predict_in_batches(comments, batch_size):\n",
    "    predictions = []\n",
    "    for i in range(0, len(comments), batch_size):\n",
    "        batch = comments[i:i + batch_size]\n",
    "        \n",
    "        # Tokenizar los comentarios y truncarlos a un máximo de 512 tokens (sin usar decode)\n",
    "        batch = [tokenizer(comment, truncation=True, max_length=512, padding=True) for comment in batch]\n",
    "        \n",
    "        # Convertir las secuencias tokenizadas a texto legible para el modelo\n",
    "        batch = [tokenizer.decode(tokens['input_ids'], skip_special_tokens=True) for tokens in batch]\n",
    "        \n",
    "        # Realizar predicciones\n",
    "        batch_predictions = classifier(batch)\n",
    "        predictions.extend(batch_predictions)\n",
    "    return predictions\n",
    "\n",
    "# Realizar las predicciones en lotes\n",
    "predictions = predict_in_batches(body, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar predicciones al DataFrame original\n",
    "filtered_comments[\"sentiment\"] = [pred[\"label\"] for pred in predictions]\n",
    "filtered_comments[\"sentiment_score\"] = [pred[\"score\"] for pred in predictions]\n",
    "\n",
    "# Definir el mapeo\n",
    "sentiment_mapping = {\"LABEL_0\": \"Neutral\", \"LABEL_1\": \"Positive\", \"LABEL_2\": \"Negative\"}\n",
    "\n",
    "# Aplicar el mapeo en el DataFrame correcto\n",
    "filtered_comments[\"sentiment\"] = filtered_comments[\"sentiment\"].map(sentiment_mapping)\n",
    "\n",
    "# Verificar resultados\n",
    "print(filtered_comments[\"sentiment\"].value_counts())\n",
    "filtered_comments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
